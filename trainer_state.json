{
  "best_metric": 0.15674851834774017,
  "best_model_checkpoint": "./results\\checkpoint-1626",
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 5420,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01845018450184502,
      "grad_norm": 2.7718570232391357,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 0.6836,
      "step": 10
    },
    {
      "epoch": 0.03690036900369004,
      "grad_norm": 3.1162219047546387,
      "learning_rate": 8.000000000000001e-07,
      "loss": 0.6858,
      "step": 20
    },
    {
      "epoch": 0.055350553505535055,
      "grad_norm": 4.694825649261475,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 0.6827,
      "step": 30
    },
    {
      "epoch": 0.07380073800738007,
      "grad_norm": 2.5776054859161377,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 0.7145,
      "step": 40
    },
    {
      "epoch": 0.09225092250922509,
      "grad_norm": 3.3528833389282227,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.6666,
      "step": 50
    },
    {
      "epoch": 0.11070110701107011,
      "grad_norm": 3.3889780044555664,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 0.6629,
      "step": 60
    },
    {
      "epoch": 0.12915129151291513,
      "grad_norm": 8.490517616271973,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 0.6472,
      "step": 70
    },
    {
      "epoch": 0.14760147601476015,
      "grad_norm": 2.8136327266693115,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 0.6292,
      "step": 80
    },
    {
      "epoch": 0.16605166051660517,
      "grad_norm": 6.562026500701904,
      "learning_rate": 3.6000000000000003e-06,
      "loss": 0.6035,
      "step": 90
    },
    {
      "epoch": 0.18450184501845018,
      "grad_norm": 3.856454372406006,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.573,
      "step": 100
    },
    {
      "epoch": 0.2029520295202952,
      "grad_norm": 3.3270184993743896,
      "learning_rate": 4.4e-06,
      "loss": 0.5404,
      "step": 110
    },
    {
      "epoch": 0.22140221402214022,
      "grad_norm": 5.080492973327637,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.4677,
      "step": 120
    },
    {
      "epoch": 0.23985239852398524,
      "grad_norm": 4.788337230682373,
      "learning_rate": 5.2e-06,
      "loss": 0.4036,
      "step": 130
    },
    {
      "epoch": 0.25830258302583026,
      "grad_norm": 5.984792709350586,
      "learning_rate": 5.600000000000001e-06,
      "loss": 0.2934,
      "step": 140
    },
    {
      "epoch": 0.2767527675276753,
      "grad_norm": 6.73891019821167,
      "learning_rate": 6e-06,
      "loss": 0.3049,
      "step": 150
    },
    {
      "epoch": 0.2952029520295203,
      "grad_norm": 3.0327744483947754,
      "learning_rate": 6.360000000000001e-06,
      "loss": 0.2519,
      "step": 160
    },
    {
      "epoch": 0.31365313653136534,
      "grad_norm": 3.0300486087799072,
      "learning_rate": 6.760000000000001e-06,
      "loss": 0.1828,
      "step": 170
    },
    {
      "epoch": 0.33210332103321033,
      "grad_norm": 5.0941362380981445,
      "learning_rate": 7.16e-06,
      "loss": 0.2234,
      "step": 180
    },
    {
      "epoch": 0.3505535055350554,
      "grad_norm": 5.633752822875977,
      "learning_rate": 7.5600000000000005e-06,
      "loss": 0.1971,
      "step": 190
    },
    {
      "epoch": 0.36900369003690037,
      "grad_norm": 16.35517692565918,
      "learning_rate": 7.92e-06,
      "loss": 0.2311,
      "step": 200
    },
    {
      "epoch": 0.3874538745387454,
      "grad_norm": 30.429489135742188,
      "learning_rate": 8.32e-06,
      "loss": 0.135,
      "step": 210
    },
    {
      "epoch": 0.4059040590405904,
      "grad_norm": 9.57572078704834,
      "learning_rate": 8.720000000000001e-06,
      "loss": 0.1038,
      "step": 220
    },
    {
      "epoch": 0.42435424354243545,
      "grad_norm": 1.606091856956482,
      "learning_rate": 9.12e-06,
      "loss": 0.0943,
      "step": 230
    },
    {
      "epoch": 0.44280442804428044,
      "grad_norm": 0.3161306083202362,
      "learning_rate": 9.52e-06,
      "loss": 0.0837,
      "step": 240
    },
    {
      "epoch": 0.4612546125461255,
      "grad_norm": 2.8108887672424316,
      "learning_rate": 9.88e-06,
      "loss": 0.1112,
      "step": 250
    },
    {
      "epoch": 0.4797047970479705,
      "grad_norm": 1.446970820426941,
      "learning_rate": 1.0280000000000002e-05,
      "loss": 0.3183,
      "step": 260
    },
    {
      "epoch": 0.4981549815498155,
      "grad_norm": 39.33361053466797,
      "learning_rate": 1.0680000000000001e-05,
      "loss": 0.3003,
      "step": 270
    },
    {
      "epoch": 0.5166051660516605,
      "grad_norm": 0.6559266448020935,
      "learning_rate": 1.1080000000000002e-05,
      "loss": 0.096,
      "step": 280
    },
    {
      "epoch": 0.5350553505535055,
      "grad_norm": 0.11672873795032501,
      "learning_rate": 1.148e-05,
      "loss": 0.1203,
      "step": 290
    },
    {
      "epoch": 0.5535055350553506,
      "grad_norm": 0.6404273509979248,
      "learning_rate": 1.188e-05,
      "loss": 0.1834,
      "step": 300
    },
    {
      "epoch": 0.5719557195571956,
      "grad_norm": 22.70252227783203,
      "learning_rate": 1.2240000000000001e-05,
      "loss": 0.1782,
      "step": 310
    },
    {
      "epoch": 0.5904059040590406,
      "grad_norm": 0.37838906049728394,
      "learning_rate": 1.2640000000000001e-05,
      "loss": 0.0038,
      "step": 320
    },
    {
      "epoch": 0.6088560885608856,
      "grad_norm": 29.303443908691406,
      "learning_rate": 1.3040000000000002e-05,
      "loss": 0.0337,
      "step": 330
    },
    {
      "epoch": 0.6273062730627307,
      "grad_norm": 4.814023494720459,
      "learning_rate": 1.3440000000000002e-05,
      "loss": 0.3228,
      "step": 340
    },
    {
      "epoch": 0.6457564575645757,
      "grad_norm": 0.10320913046598434,
      "learning_rate": 1.384e-05,
      "loss": 0.1105,
      "step": 350
    },
    {
      "epoch": 0.6642066420664207,
      "grad_norm": 7.338468551635742,
      "learning_rate": 1.4240000000000001e-05,
      "loss": 0.4957,
      "step": 360
    },
    {
      "epoch": 0.6826568265682657,
      "grad_norm": 0.06829089671373367,
      "learning_rate": 1.464e-05,
      "loss": 0.1329,
      "step": 370
    },
    {
      "epoch": 0.7011070110701108,
      "grad_norm": 0.13567042350769043,
      "learning_rate": 1.5040000000000002e-05,
      "loss": 0.1929,
      "step": 380
    },
    {
      "epoch": 0.7195571955719557,
      "grad_norm": 0.0714796856045723,
      "learning_rate": 1.544e-05,
      "loss": 0.1983,
      "step": 390
    },
    {
      "epoch": 0.7380073800738007,
      "grad_norm": 0.08438277989625931,
      "learning_rate": 1.584e-05,
      "loss": 0.0102,
      "step": 400
    },
    {
      "epoch": 0.7564575645756457,
      "grad_norm": 0.0381445549428463,
      "learning_rate": 1.6240000000000004e-05,
      "loss": 0.1172,
      "step": 410
    },
    {
      "epoch": 0.7749077490774908,
      "grad_norm": 0.03824542090296745,
      "learning_rate": 1.664e-05,
      "loss": 0.1676,
      "step": 420
    },
    {
      "epoch": 0.7933579335793358,
      "grad_norm": 0.085875503718853,
      "learning_rate": 1.704e-05,
      "loss": 0.5713,
      "step": 430
    },
    {
      "epoch": 0.8118081180811808,
      "grad_norm": 0.214529886841774,
      "learning_rate": 1.7440000000000002e-05,
      "loss": 0.2137,
      "step": 440
    },
    {
      "epoch": 0.8302583025830258,
      "grad_norm": 0.09647002071142197,
      "learning_rate": 1.7840000000000002e-05,
      "loss": 0.0561,
      "step": 450
    },
    {
      "epoch": 0.8487084870848709,
      "grad_norm": 0.10699585825204849,
      "learning_rate": 1.824e-05,
      "loss": 0.1709,
      "step": 460
    },
    {
      "epoch": 0.8671586715867159,
      "grad_norm": 6.052210330963135,
      "learning_rate": 1.864e-05,
      "loss": 0.4037,
      "step": 470
    },
    {
      "epoch": 0.8856088560885609,
      "grad_norm": 19.552656173706055,
      "learning_rate": 1.904e-05,
      "loss": 0.1658,
      "step": 480
    },
    {
      "epoch": 0.9040590405904059,
      "grad_norm": 15.35739803314209,
      "learning_rate": 1.944e-05,
      "loss": 0.1768,
      "step": 490
    },
    {
      "epoch": 0.922509225092251,
      "grad_norm": 2.041562795639038,
      "learning_rate": 1.9840000000000003e-05,
      "loss": 0.2099,
      "step": 500
    },
    {
      "epoch": 0.940959409594096,
      "grad_norm": 0.4739833176136017,
      "learning_rate": 1.9975609756097564e-05,
      "loss": 0.2051,
      "step": 510
    },
    {
      "epoch": 0.959409594095941,
      "grad_norm": 0.5929870009422302,
      "learning_rate": 1.9934959349593495e-05,
      "loss": 0.1801,
      "step": 520
    },
    {
      "epoch": 0.977859778597786,
      "grad_norm": 0.03963746875524521,
      "learning_rate": 1.9894308943089433e-05,
      "loss": 0.1265,
      "step": 530
    },
    {
      "epoch": 0.996309963099631,
      "grad_norm": 0.08174937218427658,
      "learning_rate": 1.9853658536585368e-05,
      "loss": 0.1548,
      "step": 540
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.9630996309963099,
      "eval_f1": 0.9628252788104089,
      "eval_loss": 0.1761835813522339,
      "eval_precision": 0.955719557195572,
      "eval_recall": 0.9700374531835206,
      "eval_runtime": 53.9224,
      "eval_samples_per_second": 20.103,
      "eval_steps_per_second": 1.261,
      "step": 542
    },
    {
      "epoch": 1.014760147601476,
      "grad_norm": 0.030475907027721405,
      "learning_rate": 1.9813008130081303e-05,
      "loss": 0.0102,
      "step": 550
    },
    {
      "epoch": 1.033210332103321,
      "grad_norm": 10.625811576843262,
      "learning_rate": 1.9772357723577238e-05,
      "loss": 0.34,
      "step": 560
    },
    {
      "epoch": 1.051660516605166,
      "grad_norm": 0.20091307163238525,
      "learning_rate": 1.9731707317073172e-05,
      "loss": 0.1544,
      "step": 570
    },
    {
      "epoch": 1.070110701107011,
      "grad_norm": 7.549328804016113,
      "learning_rate": 1.9691056910569107e-05,
      "loss": 0.1327,
      "step": 580
    },
    {
      "epoch": 1.088560885608856,
      "grad_norm": 0.06206154450774193,
      "learning_rate": 1.9650406504065042e-05,
      "loss": 0.0825,
      "step": 590
    },
    {
      "epoch": 1.1070110701107012,
      "grad_norm": 18.920841217041016,
      "learning_rate": 1.9609756097560977e-05,
      "loss": 0.085,
      "step": 600
    },
    {
      "epoch": 1.1254612546125462,
      "grad_norm": 0.5183169841766357,
      "learning_rate": 1.9569105691056915e-05,
      "loss": 0.1671,
      "step": 610
    },
    {
      "epoch": 1.1439114391143912,
      "grad_norm": 0.2267555147409439,
      "learning_rate": 1.9528455284552846e-05,
      "loss": 0.1214,
      "step": 620
    },
    {
      "epoch": 1.1623616236162362,
      "grad_norm": 3.3942267894744873,
      "learning_rate": 1.948780487804878e-05,
      "loss": 0.2264,
      "step": 630
    },
    {
      "epoch": 1.1808118081180812,
      "grad_norm": 0.07662992924451828,
      "learning_rate": 1.944715447154472e-05,
      "loss": 0.0107,
      "step": 640
    },
    {
      "epoch": 1.1992619926199262,
      "grad_norm": 5.66965389251709,
      "learning_rate": 1.9406504065040654e-05,
      "loss": 0.1634,
      "step": 650
    },
    {
      "epoch": 1.2177121771217712,
      "grad_norm": 0.0593252070248127,
      "learning_rate": 1.9365853658536585e-05,
      "loss": 0.0869,
      "step": 660
    },
    {
      "epoch": 1.2361623616236161,
      "grad_norm": 0.33407270908355713,
      "learning_rate": 1.9325203252032523e-05,
      "loss": 0.2399,
      "step": 670
    },
    {
      "epoch": 1.2546125461254611,
      "grad_norm": 12.209006309509277,
      "learning_rate": 1.9284552845528458e-05,
      "loss": 0.3044,
      "step": 680
    },
    {
      "epoch": 1.2730627306273063,
      "grad_norm": 0.9430513381958008,
      "learning_rate": 1.9243902439024393e-05,
      "loss": 0.1217,
      "step": 690
    },
    {
      "epoch": 1.2915129151291513,
      "grad_norm": 21.185401916503906,
      "learning_rate": 1.9203252032520327e-05,
      "loss": 0.1493,
      "step": 700
    },
    {
      "epoch": 1.3099630996309963,
      "grad_norm": 0.07900279015302658,
      "learning_rate": 1.9162601626016262e-05,
      "loss": 0.0105,
      "step": 710
    },
    {
      "epoch": 1.3284132841328413,
      "grad_norm": 0.10152572393417358,
      "learning_rate": 1.9121951219512197e-05,
      "loss": 0.32,
      "step": 720
    },
    {
      "epoch": 1.3468634686346863,
      "grad_norm": 0.2485400140285492,
      "learning_rate": 1.908130081300813e-05,
      "loss": 0.0791,
      "step": 730
    },
    {
      "epoch": 1.3653136531365313,
      "grad_norm": 5.868149757385254,
      "learning_rate": 1.9040650406504066e-05,
      "loss": 0.0881,
      "step": 740
    },
    {
      "epoch": 1.3837638376383765,
      "grad_norm": 0.37401270866394043,
      "learning_rate": 1.9e-05,
      "loss": 0.2075,
      "step": 750
    },
    {
      "epoch": 1.4022140221402215,
      "grad_norm": 0.2784838378429413,
      "learning_rate": 1.8959349593495936e-05,
      "loss": 0.0664,
      "step": 760
    },
    {
      "epoch": 1.4206642066420665,
      "grad_norm": 0.06625817716121674,
      "learning_rate": 1.891869918699187e-05,
      "loss": 0.0786,
      "step": 770
    },
    {
      "epoch": 1.4391143911439115,
      "grad_norm": 0.037525907158851624,
      "learning_rate": 1.8878048780487805e-05,
      "loss": 0.0974,
      "step": 780
    },
    {
      "epoch": 1.4575645756457565,
      "grad_norm": 0.04559699445962906,
      "learning_rate": 1.8837398373983743e-05,
      "loss": 0.0018,
      "step": 790
    },
    {
      "epoch": 1.4760147601476015,
      "grad_norm": 0.030283726751804352,
      "learning_rate": 1.8796747967479675e-05,
      "loss": 0.0236,
      "step": 800
    },
    {
      "epoch": 1.4944649446494465,
      "grad_norm": 0.034468572586774826,
      "learning_rate": 1.875609756097561e-05,
      "loss": 0.0763,
      "step": 810
    },
    {
      "epoch": 1.5129151291512914,
      "grad_norm": 0.3345026969909668,
      "learning_rate": 1.8715447154471548e-05,
      "loss": 0.08,
      "step": 820
    },
    {
      "epoch": 1.5313653136531364,
      "grad_norm": 0.5847023725509644,
      "learning_rate": 1.8674796747967482e-05,
      "loss": 0.0383,
      "step": 830
    },
    {
      "epoch": 1.5498154981549814,
      "grad_norm": 0.03139784187078476,
      "learning_rate": 1.8634146341463417e-05,
      "loss": 0.1271,
      "step": 840
    },
    {
      "epoch": 1.5682656826568264,
      "grad_norm": 2.965501070022583,
      "learning_rate": 1.8593495934959352e-05,
      "loss": 0.0219,
      "step": 850
    },
    {
      "epoch": 1.5867158671586716,
      "grad_norm": 3.1900525093078613,
      "learning_rate": 1.8552845528455287e-05,
      "loss": 0.2255,
      "step": 860
    },
    {
      "epoch": 1.6051660516605166,
      "grad_norm": 9.285211563110352,
      "learning_rate": 1.851219512195122e-05,
      "loss": 0.2639,
      "step": 870
    },
    {
      "epoch": 1.6236162361623616,
      "grad_norm": 0.07083015888929367,
      "learning_rate": 1.8471544715447156e-05,
      "loss": 0.1272,
      "step": 880
    },
    {
      "epoch": 1.6420664206642066,
      "grad_norm": 0.1465166211128235,
      "learning_rate": 1.843089430894309e-05,
      "loss": 0.0822,
      "step": 890
    },
    {
      "epoch": 1.6605166051660518,
      "grad_norm": 4.580165386199951,
      "learning_rate": 1.8390243902439026e-05,
      "loss": 0.1871,
      "step": 900
    },
    {
      "epoch": 1.6789667896678968,
      "grad_norm": 5.624207973480225,
      "learning_rate": 1.834959349593496e-05,
      "loss": 0.1368,
      "step": 910
    },
    {
      "epoch": 1.6974169741697418,
      "grad_norm": 0.07533637434244156,
      "learning_rate": 1.8308943089430895e-05,
      "loss": 0.0235,
      "step": 920
    },
    {
      "epoch": 1.7158671586715868,
      "grad_norm": 0.07093177735805511,
      "learning_rate": 1.826829268292683e-05,
      "loss": 0.0697,
      "step": 930
    },
    {
      "epoch": 1.7343173431734318,
      "grad_norm": 0.06173202395439148,
      "learning_rate": 1.8227642276422765e-05,
      "loss": 0.2083,
      "step": 940
    },
    {
      "epoch": 1.7527675276752768,
      "grad_norm": 0.14086374640464783,
      "learning_rate": 1.81869918699187e-05,
      "loss": 0.2295,
      "step": 950
    },
    {
      "epoch": 1.7712177121771218,
      "grad_norm": 2.9169652462005615,
      "learning_rate": 1.8146341463414637e-05,
      "loss": 0.0566,
      "step": 960
    },
    {
      "epoch": 1.7896678966789668,
      "grad_norm": 31.76308250427246,
      "learning_rate": 1.810569105691057e-05,
      "loss": 0.118,
      "step": 970
    },
    {
      "epoch": 1.8081180811808117,
      "grad_norm": 0.05502477288246155,
      "learning_rate": 1.8065040650406504e-05,
      "loss": 0.1807,
      "step": 980
    },
    {
      "epoch": 1.8265682656826567,
      "grad_norm": 5.976404190063477,
      "learning_rate": 1.802439024390244e-05,
      "loss": 0.1493,
      "step": 990
    },
    {
      "epoch": 1.8450184501845017,
      "grad_norm": 0.08619160205125809,
      "learning_rate": 1.7983739837398376e-05,
      "loss": 0.0065,
      "step": 1000
    },
    {
      "epoch": 1.8634686346863467,
      "grad_norm": 0.047407783567905426,
      "learning_rate": 1.7943089430894308e-05,
      "loss": 0.1356,
      "step": 1010
    },
    {
      "epoch": 1.881918819188192,
      "grad_norm": 13.485434532165527,
      "learning_rate": 1.7902439024390246e-05,
      "loss": 0.0745,
      "step": 1020
    },
    {
      "epoch": 1.900369003690037,
      "grad_norm": 0.04605187848210335,
      "learning_rate": 1.786178861788618e-05,
      "loss": 0.0872,
      "step": 1030
    },
    {
      "epoch": 1.918819188191882,
      "grad_norm": 0.08283349126577377,
      "learning_rate": 1.7821138211382115e-05,
      "loss": 0.104,
      "step": 1040
    },
    {
      "epoch": 1.937269372693727,
      "grad_norm": 25.25617027282715,
      "learning_rate": 1.778048780487805e-05,
      "loss": 0.1606,
      "step": 1050
    },
    {
      "epoch": 1.9557195571955721,
      "grad_norm": 0.030679162591695786,
      "learning_rate": 1.7739837398373985e-05,
      "loss": 0.1178,
      "step": 1060
    },
    {
      "epoch": 1.974169741697417,
      "grad_norm": 0.05882778391242027,
      "learning_rate": 1.769918699186992e-05,
      "loss": 0.0807,
      "step": 1070
    },
    {
      "epoch": 1.992619926199262,
      "grad_norm": 0.0695633590221405,
      "learning_rate": 1.7658536585365854e-05,
      "loss": 0.033,
      "step": 1080
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.966789667896679,
      "eval_f1": 0.9657794676806084,
      "eval_loss": 0.1659928262233734,
      "eval_precision": 0.9806949806949807,
      "eval_recall": 0.951310861423221,
      "eval_runtime": 54.5891,
      "eval_samples_per_second": 19.857,
      "eval_steps_per_second": 1.246,
      "step": 1084
    },
    {
      "epoch": 2.011070110701107,
      "grad_norm": 0.06364066898822784,
      "learning_rate": 1.761788617886179e-05,
      "loss": 0.0782,
      "step": 1090
    },
    {
      "epoch": 2.029520295202952,
      "grad_norm": 0.0591801218688488,
      "learning_rate": 1.7577235772357727e-05,
      "loss": 0.2066,
      "step": 1100
    },
    {
      "epoch": 2.047970479704797,
      "grad_norm": 0.13012796640396118,
      "learning_rate": 1.753658536585366e-05,
      "loss": 0.1364,
      "step": 1110
    },
    {
      "epoch": 2.066420664206642,
      "grad_norm": 0.047765184193849564,
      "learning_rate": 1.7495934959349593e-05,
      "loss": 0.0042,
      "step": 1120
    },
    {
      "epoch": 2.084870848708487,
      "grad_norm": 0.08470965921878815,
      "learning_rate": 1.745528455284553e-05,
      "loss": 0.0224,
      "step": 1130
    },
    {
      "epoch": 2.103321033210332,
      "grad_norm": 0.09098953753709793,
      "learning_rate": 1.7414634146341466e-05,
      "loss": 0.0023,
      "step": 1140
    },
    {
      "epoch": 2.121771217712177,
      "grad_norm": 0.017151637002825737,
      "learning_rate": 1.7373983739837398e-05,
      "loss": 0.0491,
      "step": 1150
    },
    {
      "epoch": 2.140221402214022,
      "grad_norm": 0.016996588557958603,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 0.001,
      "step": 1160
    },
    {
      "epoch": 2.158671586715867,
      "grad_norm": 12.097476959228516,
      "learning_rate": 1.729268292682927e-05,
      "loss": 0.0566,
      "step": 1170
    },
    {
      "epoch": 2.177121771217712,
      "grad_norm": 0.015160199254751205,
      "learning_rate": 1.7252032520325205e-05,
      "loss": 0.0042,
      "step": 1180
    },
    {
      "epoch": 2.195571955719557,
      "grad_norm": 0.012492463923990726,
      "learning_rate": 1.721138211382114e-05,
      "loss": 0.0006,
      "step": 1190
    },
    {
      "epoch": 2.2140221402214024,
      "grad_norm": 0.019064897671341896,
      "learning_rate": 1.7170731707317075e-05,
      "loss": 0.0012,
      "step": 1200
    },
    {
      "epoch": 2.2324723247232474,
      "grad_norm": 11.929718017578125,
      "learning_rate": 1.713008130081301e-05,
      "loss": 0.2338,
      "step": 1210
    },
    {
      "epoch": 2.2509225092250924,
      "grad_norm": 0.02502884529531002,
      "learning_rate": 1.7089430894308944e-05,
      "loss": 0.0019,
      "step": 1220
    },
    {
      "epoch": 2.2693726937269374,
      "grad_norm": 0.027847787365317345,
      "learning_rate": 1.704878048780488e-05,
      "loss": 0.0012,
      "step": 1230
    },
    {
      "epoch": 2.2878228782287824,
      "grad_norm": 0.025023803114891052,
      "learning_rate": 1.7008130081300817e-05,
      "loss": 0.0449,
      "step": 1240
    },
    {
      "epoch": 2.3062730627306274,
      "grad_norm": 13.120258331298828,
      "learning_rate": 1.696747967479675e-05,
      "loss": 0.1014,
      "step": 1250
    },
    {
      "epoch": 2.3247232472324724,
      "grad_norm": 0.023950019851326942,
      "learning_rate": 1.6926829268292683e-05,
      "loss": 0.0624,
      "step": 1260
    },
    {
      "epoch": 2.3431734317343174,
      "grad_norm": 0.016330264508724213,
      "learning_rate": 1.688617886178862e-05,
      "loss": 0.1702,
      "step": 1270
    },
    {
      "epoch": 2.3616236162361623,
      "grad_norm": 0.030033983290195465,
      "learning_rate": 1.6845528455284556e-05,
      "loss": 0.001,
      "step": 1280
    },
    {
      "epoch": 2.3800738007380073,
      "grad_norm": 70.6849594116211,
      "learning_rate": 1.6804878048780487e-05,
      "loss": 0.0665,
      "step": 1290
    },
    {
      "epoch": 2.3985239852398523,
      "grad_norm": 0.01173363346606493,
      "learning_rate": 1.6764227642276425e-05,
      "loss": 0.0791,
      "step": 1300
    },
    {
      "epoch": 2.4169741697416973,
      "grad_norm": 0.08353931456804276,
      "learning_rate": 1.672357723577236e-05,
      "loss": 0.1122,
      "step": 1310
    },
    {
      "epoch": 2.4354243542435423,
      "grad_norm": 12.32367992401123,
      "learning_rate": 1.6682926829268295e-05,
      "loss": 0.0981,
      "step": 1320
    },
    {
      "epoch": 2.4538745387453873,
      "grad_norm": 0.025135470554232597,
      "learning_rate": 1.664227642276423e-05,
      "loss": 0.001,
      "step": 1330
    },
    {
      "epoch": 2.4723247232472323,
      "grad_norm": 0.028958531096577644,
      "learning_rate": 1.6601626016260164e-05,
      "loss": 0.2158,
      "step": 1340
    },
    {
      "epoch": 2.4907749077490777,
      "grad_norm": 45.903690338134766,
      "learning_rate": 1.65609756097561e-05,
      "loss": 0.0301,
      "step": 1350
    },
    {
      "epoch": 2.5092250922509223,
      "grad_norm": 0.026061853393912315,
      "learning_rate": 1.6520325203252034e-05,
      "loss": 0.0019,
      "step": 1360
    },
    {
      "epoch": 2.5276752767527677,
      "grad_norm": 0.03845697641372681,
      "learning_rate": 1.647967479674797e-05,
      "loss": 0.1356,
      "step": 1370
    },
    {
      "epoch": 2.5461254612546127,
      "grad_norm": 0.030573049560189247,
      "learning_rate": 1.6439024390243903e-05,
      "loss": 0.0257,
      "step": 1380
    },
    {
      "epoch": 2.5645756457564577,
      "grad_norm": 0.026881393045186996,
      "learning_rate": 1.6398373983739838e-05,
      "loss": 0.1689,
      "step": 1390
    },
    {
      "epoch": 2.5830258302583027,
      "grad_norm": 0.03875619173049927,
      "learning_rate": 1.6357723577235773e-05,
      "loss": 0.0863,
      "step": 1400
    },
    {
      "epoch": 2.6014760147601477,
      "grad_norm": 0.057705920189619064,
      "learning_rate": 1.6317073170731708e-05,
      "loss": 0.0605,
      "step": 1410
    },
    {
      "epoch": 2.6199261992619927,
      "grad_norm": 3.8328044414520264,
      "learning_rate": 1.6276422764227642e-05,
      "loss": 0.0793,
      "step": 1420
    },
    {
      "epoch": 2.6383763837638377,
      "grad_norm": 44.319549560546875,
      "learning_rate": 1.6235772357723577e-05,
      "loss": 0.112,
      "step": 1430
    },
    {
      "epoch": 2.6568265682656826,
      "grad_norm": 0.048650629818439484,
      "learning_rate": 1.6195121951219515e-05,
      "loss": 0.0507,
      "step": 1440
    },
    {
      "epoch": 2.6752767527675276,
      "grad_norm": 0.03366660699248314,
      "learning_rate": 1.615447154471545e-05,
      "loss": 0.1606,
      "step": 1450
    },
    {
      "epoch": 2.6937269372693726,
      "grad_norm": 17.434293746948242,
      "learning_rate": 1.611382113821138e-05,
      "loss": 0.1662,
      "step": 1460
    },
    {
      "epoch": 2.7121771217712176,
      "grad_norm": 0.06573693454265594,
      "learning_rate": 1.607317073170732e-05,
      "loss": 0.0025,
      "step": 1470
    },
    {
      "epoch": 2.7306273062730626,
      "grad_norm": 31.378631591796875,
      "learning_rate": 1.6032520325203254e-05,
      "loss": 0.1194,
      "step": 1480
    },
    {
      "epoch": 2.7490774907749076,
      "grad_norm": 0.04818784445524216,
      "learning_rate": 1.599186991869919e-05,
      "loss": 0.1156,
      "step": 1490
    },
    {
      "epoch": 2.767527675276753,
      "grad_norm": 0.04323100298643112,
      "learning_rate": 1.5951219512195124e-05,
      "loss": 0.0929,
      "step": 1500
    },
    {
      "epoch": 2.7859778597785976,
      "grad_norm": 0.13358624279499054,
      "learning_rate": 1.591056910569106e-05,
      "loss": 0.0329,
      "step": 1510
    },
    {
      "epoch": 2.804428044280443,
      "grad_norm": 0.07078008353710175,
      "learning_rate": 1.5869918699186993e-05,
      "loss": 0.003,
      "step": 1520
    },
    {
      "epoch": 2.8228782287822876,
      "grad_norm": 0.031425077468156815,
      "learning_rate": 1.5829268292682928e-05,
      "loss": 0.0882,
      "step": 1530
    },
    {
      "epoch": 2.841328413284133,
      "grad_norm": 0.08348224312067032,
      "learning_rate": 1.5788617886178863e-05,
      "loss": 0.0623,
      "step": 1540
    },
    {
      "epoch": 2.859778597785978,
      "grad_norm": 0.13050420582294464,
      "learning_rate": 1.5747967479674797e-05,
      "loss": 0.098,
      "step": 1550
    },
    {
      "epoch": 2.878228782287823,
      "grad_norm": 0.039748940616846085,
      "learning_rate": 1.5707317073170732e-05,
      "loss": 0.0034,
      "step": 1560
    },
    {
      "epoch": 2.896678966789668,
      "grad_norm": 0.03463635966181755,
      "learning_rate": 1.5666666666666667e-05,
      "loss": 0.0982,
      "step": 1570
    },
    {
      "epoch": 2.915129151291513,
      "grad_norm": 0.023048991337418556,
      "learning_rate": 1.56260162601626e-05,
      "loss": 0.0021,
      "step": 1580
    },
    {
      "epoch": 2.933579335793358,
      "grad_norm": 3.4658379554748535,
      "learning_rate": 1.558536585365854e-05,
      "loss": 0.1916,
      "step": 1590
    },
    {
      "epoch": 2.952029520295203,
      "grad_norm": 0.051736362278461456,
      "learning_rate": 1.554471544715447e-05,
      "loss": 0.0779,
      "step": 1600
    },
    {
      "epoch": 2.970479704797048,
      "grad_norm": 5.670722961425781,
      "learning_rate": 1.5504065040650406e-05,
      "loss": 0.126,
      "step": 1610
    },
    {
      "epoch": 2.988929889298893,
      "grad_norm": 3.145740032196045,
      "learning_rate": 1.5463414634146344e-05,
      "loss": 0.1376,
      "step": 1620
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.966789667896679,
      "eval_f1": 0.9662921348314607,
      "eval_loss": 0.15674851834774017,
      "eval_precision": 0.9662921348314607,
      "eval_recall": 0.9662921348314607,
      "eval_runtime": 54.2232,
      "eval_samples_per_second": 19.991,
      "eval_steps_per_second": 1.254,
      "step": 1626
    },
    {
      "epoch": 3.007380073800738,
      "grad_norm": 0.08104047924280167,
      "learning_rate": 1.542276422764228e-05,
      "loss": 0.0502,
      "step": 1630
    },
    {
      "epoch": 3.025830258302583,
      "grad_norm": 5.583915710449219,
      "learning_rate": 1.5382113821138213e-05,
      "loss": 0.0775,
      "step": 1640
    },
    {
      "epoch": 3.044280442804428,
      "grad_norm": 0.05808492377400398,
      "learning_rate": 1.5341463414634148e-05,
      "loss": 0.1224,
      "step": 1650
    },
    {
      "epoch": 3.062730627306273,
      "grad_norm": 0.046353284269571304,
      "learning_rate": 1.5300813008130083e-05,
      "loss": 0.143,
      "step": 1660
    },
    {
      "epoch": 3.081180811808118,
      "grad_norm": 0.02810814417898655,
      "learning_rate": 1.5260162601626018e-05,
      "loss": 0.0408,
      "step": 1670
    },
    {
      "epoch": 3.0996309963099633,
      "grad_norm": 0.07583756744861603,
      "learning_rate": 1.5219512195121952e-05,
      "loss": 0.0448,
      "step": 1680
    },
    {
      "epoch": 3.1180811808118083,
      "grad_norm": 3.214555263519287,
      "learning_rate": 1.5178861788617887e-05,
      "loss": 0.0845,
      "step": 1690
    },
    {
      "epoch": 3.1365313653136533,
      "grad_norm": 0.08067527413368225,
      "learning_rate": 1.5138211382113824e-05,
      "loss": 0.0217,
      "step": 1700
    },
    {
      "epoch": 3.1549815498154983,
      "grad_norm": 0.03744988888502121,
      "learning_rate": 1.5097560975609757e-05,
      "loss": 0.021,
      "step": 1710
    },
    {
      "epoch": 3.1734317343173433,
      "grad_norm": 0.6212438941001892,
      "learning_rate": 1.5056910569105691e-05,
      "loss": 0.0016,
      "step": 1720
    },
    {
      "epoch": 3.1918819188191883,
      "grad_norm": 15.041435241699219,
      "learning_rate": 1.5016260162601628e-05,
      "loss": 0.1096,
      "step": 1730
    },
    {
      "epoch": 3.2103321033210332,
      "grad_norm": 0.01959347352385521,
      "learning_rate": 1.4975609756097563e-05,
      "loss": 0.0011,
      "step": 1740
    },
    {
      "epoch": 3.2287822878228782,
      "grad_norm": 0.0339634008705616,
      "learning_rate": 1.4934959349593496e-05,
      "loss": 0.001,
      "step": 1750
    },
    {
      "epoch": 3.2472324723247232,
      "grad_norm": 0.01973160356283188,
      "learning_rate": 1.4894308943089432e-05,
      "loss": 0.001,
      "step": 1760
    },
    {
      "epoch": 3.265682656826568,
      "grad_norm": 0.02421073243021965,
      "learning_rate": 1.4853658536585367e-05,
      "loss": 0.0854,
      "step": 1770
    },
    {
      "epoch": 3.284132841328413,
      "grad_norm": 0.02765919268131256,
      "learning_rate": 1.4813008130081302e-05,
      "loss": 0.0009,
      "step": 1780
    },
    {
      "epoch": 3.302583025830258,
      "grad_norm": 3.5804920196533203,
      "learning_rate": 1.4772357723577238e-05,
      "loss": 0.1561,
      "step": 1790
    },
    {
      "epoch": 3.321033210332103,
      "grad_norm": 0.041864506900310516,
      "learning_rate": 1.4731707317073171e-05,
      "loss": 0.0677,
      "step": 1800
    },
    {
      "epoch": 3.339483394833948,
      "grad_norm": 0.024725893512368202,
      "learning_rate": 1.4691056910569106e-05,
      "loss": 0.0032,
      "step": 1810
    },
    {
      "epoch": 3.357933579335793,
      "grad_norm": 7.070119380950928,
      "learning_rate": 1.4650406504065042e-05,
      "loss": 0.0773,
      "step": 1820
    },
    {
      "epoch": 3.376383763837638,
      "grad_norm": 0.03413453698158264,
      "learning_rate": 1.4609756097560977e-05,
      "loss": 0.0643,
      "step": 1830
    },
    {
      "epoch": 3.3948339483394836,
      "grad_norm": 0.02849072962999344,
      "learning_rate": 1.4569105691056913e-05,
      "loss": 0.0088,
      "step": 1840
    },
    {
      "epoch": 3.4132841328413286,
      "grad_norm": 0.07725588977336884,
      "learning_rate": 1.4528455284552846e-05,
      "loss": 0.0749,
      "step": 1850
    },
    {
      "epoch": 3.4317343173431736,
      "grad_norm": 0.028743254020810127,
      "learning_rate": 1.4487804878048781e-05,
      "loss": 0.0019,
      "step": 1860
    },
    {
      "epoch": 3.4501845018450186,
      "grad_norm": 0.023623093962669373,
      "learning_rate": 1.4447154471544718e-05,
      "loss": 0.0013,
      "step": 1870
    },
    {
      "epoch": 3.4686346863468636,
      "grad_norm": 3.1329541206359863,
      "learning_rate": 1.4406504065040652e-05,
      "loss": 0.0819,
      "step": 1880
    },
    {
      "epoch": 3.4870848708487086,
      "grad_norm": 0.022845856845378876,
      "learning_rate": 1.4365853658536585e-05,
      "loss": 0.0013,
      "step": 1890
    },
    {
      "epoch": 3.5055350553505535,
      "grad_norm": 0.0353131964802742,
      "learning_rate": 1.4325203252032522e-05,
      "loss": 0.0046,
      "step": 1900
    },
    {
      "epoch": 3.5239852398523985,
      "grad_norm": 0.006678307894617319,
      "learning_rate": 1.4284552845528457e-05,
      "loss": 0.001,
      "step": 1910
    },
    {
      "epoch": 3.5424354243542435,
      "grad_norm": 0.01629803143441677,
      "learning_rate": 1.4243902439024391e-05,
      "loss": 0.0016,
      "step": 1920
    },
    {
      "epoch": 3.5608856088560885,
      "grad_norm": 0.01244909968227148,
      "learning_rate": 1.4203252032520328e-05,
      "loss": 0.0083,
      "step": 1930
    },
    {
      "epoch": 3.5793357933579335,
      "grad_norm": 0.06668201088905334,
      "learning_rate": 1.416260162601626e-05,
      "loss": 0.0007,
      "step": 1940
    },
    {
      "epoch": 3.5977859778597785,
      "grad_norm": 1.787651777267456,
      "learning_rate": 1.4121951219512196e-05,
      "loss": 0.2072,
      "step": 1950
    },
    {
      "epoch": 3.6162361623616235,
      "grad_norm": 0.016430223360657692,
      "learning_rate": 1.4081300813008132e-05,
      "loss": 0.1013,
      "step": 1960
    },
    {
      "epoch": 3.6346863468634685,
      "grad_norm": 0.030151406303048134,
      "learning_rate": 1.4040650406504067e-05,
      "loss": 0.0342,
      "step": 1970
    },
    {
      "epoch": 3.6531365313653135,
      "grad_norm": 0.01955675520002842,
      "learning_rate": 1.4e-05,
      "loss": 0.001,
      "step": 1980
    },
    {
      "epoch": 3.671586715867159,
      "grad_norm": 0.019082387909293175,
      "learning_rate": 1.3959349593495936e-05,
      "loss": 0.0447,
      "step": 1990
    },
    {
      "epoch": 3.6900369003690034,
      "grad_norm": 0.02246023342013359,
      "learning_rate": 1.3918699186991871e-05,
      "loss": 0.1916,
      "step": 2000
    },
    {
      "epoch": 3.708487084870849,
      "grad_norm": 4.639730930328369,
      "learning_rate": 1.3878048780487806e-05,
      "loss": 0.0848,
      "step": 2010
    },
    {
      "epoch": 3.726937269372694,
      "grad_norm": 0.09713561832904816,
      "learning_rate": 1.3837398373983742e-05,
      "loss": 0.0015,
      "step": 2020
    },
    {
      "epoch": 3.745387453874539,
      "grad_norm": 0.036473874002695084,
      "learning_rate": 1.3796747967479675e-05,
      "loss": 0.003,
      "step": 2030
    },
    {
      "epoch": 3.763837638376384,
      "grad_norm": 0.03057626448571682,
      "learning_rate": 1.375609756097561e-05,
      "loss": 0.1147,
      "step": 2040
    },
    {
      "epoch": 3.782287822878229,
      "grad_norm": 0.023928700014948845,
      "learning_rate": 1.3715447154471546e-05,
      "loss": 0.0137,
      "step": 2050
    },
    {
      "epoch": 3.800738007380074,
      "grad_norm": 28.633642196655273,
      "learning_rate": 1.3674796747967481e-05,
      "loss": 0.0408,
      "step": 2060
    },
    {
      "epoch": 3.819188191881919,
      "grad_norm": 0.035183489322662354,
      "learning_rate": 1.3634146341463418e-05,
      "loss": 0.0013,
      "step": 2070
    },
    {
      "epoch": 3.837638376383764,
      "grad_norm": 0.016445182263851166,
      "learning_rate": 1.359349593495935e-05,
      "loss": 0.0007,
      "step": 2080
    },
    {
      "epoch": 3.856088560885609,
      "grad_norm": 0.020225979387760162,
      "learning_rate": 1.3552845528455285e-05,
      "loss": 0.0899,
      "step": 2090
    },
    {
      "epoch": 3.874538745387454,
      "grad_norm": 0.01685221679508686,
      "learning_rate": 1.3512195121951222e-05,
      "loss": 0.0015,
      "step": 2100
    },
    {
      "epoch": 3.892988929889299,
      "grad_norm": 0.019902842119336128,
      "learning_rate": 1.3471544715447157e-05,
      "loss": 0.1127,
      "step": 2110
    },
    {
      "epoch": 3.911439114391144,
      "grad_norm": 0.01809600740671158,
      "learning_rate": 1.343089430894309e-05,
      "loss": 0.001,
      "step": 2120
    },
    {
      "epoch": 3.9298892988929888,
      "grad_norm": 0.026476044207811356,
      "learning_rate": 1.3390243902439026e-05,
      "loss": 0.0013,
      "step": 2130
    },
    {
      "epoch": 3.948339483394834,
      "grad_norm": 0.02681940421462059,
      "learning_rate": 1.334959349593496e-05,
      "loss": 0.0015,
      "step": 2140
    },
    {
      "epoch": 3.9667896678966788,
      "grad_norm": 0.017227236181497574,
      "learning_rate": 1.3308943089430894e-05,
      "loss": 0.0009,
      "step": 2150
    },
    {
      "epoch": 3.985239852398524,
      "grad_norm": 0.014880191534757614,
      "learning_rate": 1.326829268292683e-05,
      "loss": 0.0008,
      "step": 2160
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.9630996309963099,
      "eval_f1": 0.9630314232902034,
      "eval_loss": 0.21496237814426422,
      "eval_precision": 0.9507299270072993,
      "eval_recall": 0.9756554307116105,
      "eval_runtime": 53.8109,
      "eval_samples_per_second": 20.145,
      "eval_steps_per_second": 1.264,
      "step": 2168
    },
    {
      "epoch": 4.003690036900369,
      "grad_norm": 0.01564590074121952,
      "learning_rate": 1.3227642276422765e-05,
      "loss": 0.076,
      "step": 2170
    },
    {
      "epoch": 4.022140221402214,
      "grad_norm": 0.011412902735173702,
      "learning_rate": 1.31869918699187e-05,
      "loss": 0.0017,
      "step": 2180
    },
    {
      "epoch": 4.040590405904059,
      "grad_norm": 0.1491246372461319,
      "learning_rate": 1.3146341463414636e-05,
      "loss": 0.0022,
      "step": 2190
    },
    {
      "epoch": 4.059040590405904,
      "grad_norm": 0.00927724502980709,
      "learning_rate": 1.310569105691057e-05,
      "loss": 0.083,
      "step": 2200
    },
    {
      "epoch": 4.077490774907749,
      "grad_norm": 0.010958055034279823,
      "learning_rate": 1.3065040650406504e-05,
      "loss": 0.0005,
      "step": 2210
    },
    {
      "epoch": 4.095940959409594,
      "grad_norm": 0.008858772926032543,
      "learning_rate": 1.302439024390244e-05,
      "loss": 0.0005,
      "step": 2220
    },
    {
      "epoch": 4.114391143911439,
      "grad_norm": 0.01712764985859394,
      "learning_rate": 1.2983739837398375e-05,
      "loss": 0.1897,
      "step": 2230
    },
    {
      "epoch": 4.132841328413284,
      "grad_norm": 0.01916477084159851,
      "learning_rate": 1.2943089430894308e-05,
      "loss": 0.0005,
      "step": 2240
    },
    {
      "epoch": 4.1512915129151295,
      "grad_norm": 0.019240092486143112,
      "learning_rate": 1.2902439024390245e-05,
      "loss": 0.0925,
      "step": 2250
    },
    {
      "epoch": 4.169741697416974,
      "grad_norm": 0.02321411669254303,
      "learning_rate": 1.286178861788618e-05,
      "loss": 0.001,
      "step": 2260
    },
    {
      "epoch": 4.1881918819188195,
      "grad_norm": 3.6585559844970703,
      "learning_rate": 1.2821138211382116e-05,
      "loss": 0.0804,
      "step": 2270
    },
    {
      "epoch": 4.206642066420664,
      "grad_norm": 0.13123361766338348,
      "learning_rate": 1.278048780487805e-05,
      "loss": 0.0535,
      "step": 2280
    },
    {
      "epoch": 4.2250922509225095,
      "grad_norm": 0.030328258872032166,
      "learning_rate": 1.2739837398373984e-05,
      "loss": 0.0039,
      "step": 2290
    },
    {
      "epoch": 4.243542435424354,
      "grad_norm": 28.901830673217773,
      "learning_rate": 1.269918699186992e-05,
      "loss": 0.0297,
      "step": 2300
    },
    {
      "epoch": 4.2619926199261995,
      "grad_norm": 0.020425308495759964,
      "learning_rate": 1.2658536585365855e-05,
      "loss": 0.0008,
      "step": 2310
    },
    {
      "epoch": 4.280442804428044,
      "grad_norm": 0.03381729498505592,
      "learning_rate": 1.261788617886179e-05,
      "loss": 0.1621,
      "step": 2320
    },
    {
      "epoch": 4.2988929889298895,
      "grad_norm": 0.09931407123804092,
      "learning_rate": 1.2577235772357726e-05,
      "loss": 0.002,
      "step": 2330
    },
    {
      "epoch": 4.317343173431734,
      "grad_norm": 0.038234107196331024,
      "learning_rate": 1.2536585365853659e-05,
      "loss": 0.0138,
      "step": 2340
    },
    {
      "epoch": 4.3357933579335795,
      "grad_norm": 0.027215896174311638,
      "learning_rate": 1.2495934959349594e-05,
      "loss": 0.0012,
      "step": 2350
    },
    {
      "epoch": 4.354243542435424,
      "grad_norm": 0.016607193276286125,
      "learning_rate": 1.245528455284553e-05,
      "loss": 0.0179,
      "step": 2360
    },
    {
      "epoch": 4.372693726937269,
      "grad_norm": 0.010294255800545216,
      "learning_rate": 1.2414634146341465e-05,
      "loss": 0.0007,
      "step": 2370
    },
    {
      "epoch": 4.391143911439114,
      "grad_norm": 0.10679040104150772,
      "learning_rate": 1.2373983739837398e-05,
      "loss": 0.0008,
      "step": 2380
    },
    {
      "epoch": 4.409594095940959,
      "grad_norm": 0.019867250695824623,
      "learning_rate": 1.2333333333333334e-05,
      "loss": 0.0009,
      "step": 2390
    },
    {
      "epoch": 4.428044280442805,
      "grad_norm": 0.017211973667144775,
      "learning_rate": 1.2292682926829269e-05,
      "loss": 0.0007,
      "step": 2400
    },
    {
      "epoch": 4.446494464944649,
      "grad_norm": 0.006825411692261696,
      "learning_rate": 1.2252032520325204e-05,
      "loss": 0.0005,
      "step": 2410
    },
    {
      "epoch": 4.464944649446495,
      "grad_norm": 0.008193587884306908,
      "learning_rate": 1.221138211382114e-05,
      "loss": 0.0004,
      "step": 2420
    },
    {
      "epoch": 4.483394833948339,
      "grad_norm": 0.008430973626673222,
      "learning_rate": 1.2170731707317073e-05,
      "loss": 0.0004,
      "step": 2430
    },
    {
      "epoch": 4.501845018450185,
      "grad_norm": 1.3691368103027344,
      "learning_rate": 1.2130081300813008e-05,
      "loss": 0.1722,
      "step": 2440
    },
    {
      "epoch": 4.520295202952029,
      "grad_norm": 0.00843881443142891,
      "learning_rate": 1.2089430894308945e-05,
      "loss": 0.0005,
      "step": 2450
    },
    {
      "epoch": 4.538745387453875,
      "grad_norm": 0.008551253005862236,
      "learning_rate": 1.204878048780488e-05,
      "loss": 0.0004,
      "step": 2460
    },
    {
      "epoch": 4.557195571955719,
      "grad_norm": 0.008060253225266933,
      "learning_rate": 1.2008130081300816e-05,
      "loss": 0.0004,
      "step": 2470
    },
    {
      "epoch": 4.575645756457565,
      "grad_norm": 0.009583525359630585,
      "learning_rate": 1.1971544715447156e-05,
      "loss": 0.0215,
      "step": 2480
    },
    {
      "epoch": 4.594095940959409,
      "grad_norm": 0.012978211976587772,
      "learning_rate": 1.193089430894309e-05,
      "loss": 0.0982,
      "step": 2490
    },
    {
      "epoch": 4.612546125461255,
      "grad_norm": 0.014941920526325703,
      "learning_rate": 1.1890243902439025e-05,
      "loss": 0.0005,
      "step": 2500
    },
    {
      "epoch": 4.630996309963099,
      "grad_norm": 0.021080488339066505,
      "learning_rate": 1.1849593495934962e-05,
      "loss": 0.0005,
      "step": 2510
    },
    {
      "epoch": 4.649446494464945,
      "grad_norm": 0.011205586604773998,
      "learning_rate": 1.1808943089430895e-05,
      "loss": 0.0005,
      "step": 2520
    },
    {
      "epoch": 4.667896678966789,
      "grad_norm": 0.010663479566574097,
      "learning_rate": 1.176829268292683e-05,
      "loss": 0.0084,
      "step": 2530
    },
    {
      "epoch": 4.686346863468635,
      "grad_norm": 0.013759673573076725,
      "learning_rate": 1.1727642276422766e-05,
      "loss": 0.0005,
      "step": 2540
    },
    {
      "epoch": 4.70479704797048,
      "grad_norm": 0.007833440788090229,
      "learning_rate": 1.16869918699187e-05,
      "loss": 0.0004,
      "step": 2550
    },
    {
      "epoch": 4.723247232472325,
      "grad_norm": 0.009311779402196407,
      "learning_rate": 1.1646341463414634e-05,
      "loss": 0.0004,
      "step": 2560
    },
    {
      "epoch": 4.741697416974169,
      "grad_norm": 5.516001224517822,
      "learning_rate": 1.160569105691057e-05,
      "loss": 0.0014,
      "step": 2570
    },
    {
      "epoch": 4.760147601476015,
      "grad_norm": 0.007070022169500589,
      "learning_rate": 1.1565040650406505e-05,
      "loss": 0.0907,
      "step": 2580
    },
    {
      "epoch": 4.77859778597786,
      "grad_norm": 0.014512872323393822,
      "learning_rate": 1.152439024390244e-05,
      "loss": 0.0005,
      "step": 2590
    },
    {
      "epoch": 4.797047970479705,
      "grad_norm": 0.06225665658712387,
      "learning_rate": 1.1483739837398376e-05,
      "loss": 0.0005,
      "step": 2600
    },
    {
      "epoch": 4.81549815498155,
      "grad_norm": 0.012357697822153568,
      "learning_rate": 1.1443089430894309e-05,
      "loss": 0.105,
      "step": 2610
    },
    {
      "epoch": 4.833948339483395,
      "grad_norm": 0.07607714831829071,
      "learning_rate": 1.1402439024390244e-05,
      "loss": 0.0952,
      "step": 2620
    },
    {
      "epoch": 4.85239852398524,
      "grad_norm": 1.1590137481689453,
      "learning_rate": 1.136178861788618e-05,
      "loss": 0.0019,
      "step": 2630
    },
    {
      "epoch": 4.870848708487085,
      "grad_norm": 6.6278181076049805,
      "learning_rate": 1.1321138211382115e-05,
      "loss": 0.026,
      "step": 2640
    },
    {
      "epoch": 4.88929889298893,
      "grad_norm": 0.015829050913453102,
      "learning_rate": 1.1280487804878048e-05,
      "loss": 0.0005,
      "step": 2650
    },
    {
      "epoch": 4.907749077490775,
      "grad_norm": 0.012627197429537773,
      "learning_rate": 1.1239837398373984e-05,
      "loss": 0.0005,
      "step": 2660
    },
    {
      "epoch": 4.92619926199262,
      "grad_norm": 0.007831682451069355,
      "learning_rate": 1.1199186991869919e-05,
      "loss": 0.1209,
      "step": 2670
    },
    {
      "epoch": 4.944649446494465,
      "grad_norm": 0.01179618202149868,
      "learning_rate": 1.1158536585365856e-05,
      "loss": 0.0911,
      "step": 2680
    },
    {
      "epoch": 4.96309963099631,
      "grad_norm": 0.008578823879361153,
      "learning_rate": 1.111788617886179e-05,
      "loss": 0.0005,
      "step": 2690
    },
    {
      "epoch": 4.9815498154981555,
      "grad_norm": 0.015167533420026302,
      "learning_rate": 1.1077235772357723e-05,
      "loss": 0.0006,
      "step": 2700
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.03113901987671852,
      "learning_rate": 1.103658536585366e-05,
      "loss": 0.1774,
      "step": 2710
    },
    {
      "epoch": 5.0,
      "eval_accuracy": 0.9704797047970479,
      "eval_f1": 0.9700934579439252,
      "eval_loss": 0.19460120797157288,
      "eval_precision": 0.9682835820895522,
      "eval_recall": 0.9719101123595506,
      "eval_runtime": 51.6366,
      "eval_samples_per_second": 20.993,
      "eval_steps_per_second": 1.317,
      "step": 2710
    },
    {
      "epoch": 5.018450184501845,
      "grad_norm": 0.014578226022422314,
      "learning_rate": 1.0995934959349595e-05,
      "loss": 0.0009,
      "step": 2720
    },
    {
      "epoch": 5.03690036900369,
      "grad_norm": 0.012540492229163647,
      "learning_rate": 1.095528455284553e-05,
      "loss": 0.0009,
      "step": 2730
    },
    {
      "epoch": 5.055350553505535,
      "grad_norm": 0.03702636435627937,
      "learning_rate": 1.0914634146341466e-05,
      "loss": 0.0009,
      "step": 2740
    },
    {
      "epoch": 5.07380073800738,
      "grad_norm": 0.009745021350681782,
      "learning_rate": 1.0873983739837399e-05,
      "loss": 0.0007,
      "step": 2750
    },
    {
      "epoch": 5.092250922509225,
      "grad_norm": 0.016752149909734726,
      "learning_rate": 1.0833333333333334e-05,
      "loss": 0.0006,
      "step": 2760
    },
    {
      "epoch": 5.11070110701107,
      "grad_norm": 0.016307121142745018,
      "learning_rate": 1.079268292682927e-05,
      "loss": 0.0909,
      "step": 2770
    },
    {
      "epoch": 5.129151291512915,
      "grad_norm": 0.012027877382934093,
      "learning_rate": 1.0752032520325205e-05,
      "loss": 0.0006,
      "step": 2780
    },
    {
      "epoch": 5.14760147601476,
      "grad_norm": 0.020347850397229195,
      "learning_rate": 1.0711382113821138e-05,
      "loss": 0.0127,
      "step": 2790
    },
    {
      "epoch": 5.166051660516605,
      "grad_norm": 0.022450074553489685,
      "learning_rate": 1.0670731707317074e-05,
      "loss": 0.0325,
      "step": 2800
    },
    {
      "epoch": 5.18450184501845,
      "grad_norm": 0.015353030525147915,
      "learning_rate": 1.0630081300813009e-05,
      "loss": 0.0008,
      "step": 2810
    },
    {
      "epoch": 5.202952029520295,
      "grad_norm": 0.017990252003073692,
      "learning_rate": 1.0589430894308944e-05,
      "loss": 0.091,
      "step": 2820
    },
    {
      "epoch": 5.22140221402214,
      "grad_norm": 0.02623528428375721,
      "learning_rate": 1.054878048780488e-05,
      "loss": 0.0006,
      "step": 2830
    },
    {
      "epoch": 5.239852398523985,
      "grad_norm": 0.012887918390333652,
      "learning_rate": 1.0508130081300813e-05,
      "loss": 0.0006,
      "step": 2840
    },
    {
      "epoch": 5.25830258302583,
      "grad_norm": 0.010507936589419842,
      "learning_rate": 1.0467479674796748e-05,
      "loss": 0.0318,
      "step": 2850
    },
    {
      "epoch": 5.276752767527675,
      "grad_norm": 0.016861695796251297,
      "learning_rate": 1.0426829268292684e-05,
      "loss": 0.0007,
      "step": 2860
    },
    {
      "epoch": 5.29520295202952,
      "grad_norm": 0.04152367636561394,
      "learning_rate": 1.0386178861788619e-05,
      "loss": 0.0511,
      "step": 2870
    },
    {
      "epoch": 5.313653136531365,
      "grad_norm": 0.014417772181332111,
      "learning_rate": 1.0345528455284556e-05,
      "loss": 0.0008,
      "step": 2880
    },
    {
      "epoch": 5.332103321033211,
      "grad_norm": 0.015326387248933315,
      "learning_rate": 1.0304878048780489e-05,
      "loss": 0.0004,
      "step": 2890
    },
    {
      "epoch": 5.350553505535055,
      "grad_norm": 0.009004160761833191,
      "learning_rate": 1.0264227642276423e-05,
      "loss": 0.0004,
      "step": 2900
    },
    {
      "epoch": 5.369003690036901,
      "grad_norm": 0.01122031919658184,
      "learning_rate": 1.022357723577236e-05,
      "loss": 0.0004,
      "step": 2910
    },
    {
      "epoch": 5.387453874538745,
      "grad_norm": 0.007905247621238232,
      "learning_rate": 1.0182926829268294e-05,
      "loss": 0.1763,
      "step": 2920
    },
    {
      "epoch": 5.405904059040591,
      "grad_norm": 7.446337699890137,
      "learning_rate": 1.0142276422764228e-05,
      "loss": 0.0022,
      "step": 2930
    },
    {
      "epoch": 5.424354243542435,
      "grad_norm": 0.011646327562630177,
      "learning_rate": 1.0101626016260164e-05,
      "loss": 0.0006,
      "step": 2940
    },
    {
      "epoch": 5.442804428044281,
      "grad_norm": 0.015275933779776096,
      "learning_rate": 1.0060975609756099e-05,
      "loss": 0.0968,
      "step": 2950
    },
    {
      "epoch": 5.461254612546125,
      "grad_norm": 0.016734542325139046,
      "learning_rate": 1.0020325203252033e-05,
      "loss": 0.0029,
      "step": 2960
    },
    {
      "epoch": 5.479704797047971,
      "grad_norm": 0.014845280908048153,
      "learning_rate": 9.979674796747968e-06,
      "loss": 0.0024,
      "step": 2970
    },
    {
      "epoch": 5.498154981549815,
      "grad_norm": 0.030881762504577637,
      "learning_rate": 9.939024390243903e-06,
      "loss": 0.0846,
      "step": 2980
    },
    {
      "epoch": 5.516605166051661,
      "grad_norm": 3.378476142883301,
      "learning_rate": 9.898373983739838e-06,
      "loss": 0.0017,
      "step": 2990
    },
    {
      "epoch": 5.535055350553505,
      "grad_norm": 0.05250637233257294,
      "learning_rate": 9.857723577235772e-06,
      "loss": 0.053,
      "step": 3000
    },
    {
      "epoch": 5.553505535055351,
      "grad_norm": 0.017331646755337715,
      "learning_rate": 9.817073170731707e-06,
      "loss": 0.0403,
      "step": 3010
    },
    {
      "epoch": 5.571955719557195,
      "grad_norm": 0.011354292742908001,
      "learning_rate": 9.776422764227644e-06,
      "loss": 0.0008,
      "step": 3020
    },
    {
      "epoch": 5.590405904059041,
      "grad_norm": 0.015542857348918915,
      "learning_rate": 9.735772357723578e-06,
      "loss": 0.0006,
      "step": 3030
    },
    {
      "epoch": 5.608856088560886,
      "grad_norm": 0.010550481267273426,
      "learning_rate": 9.695121951219513e-06,
      "loss": 0.1002,
      "step": 3040
    },
    {
      "epoch": 5.627306273062731,
      "grad_norm": 0.021424712613224983,
      "learning_rate": 9.654471544715448e-06,
      "loss": 0.0008,
      "step": 3050
    },
    {
      "epoch": 5.645756457564576,
      "grad_norm": 0.37491801381111145,
      "learning_rate": 9.613821138211383e-06,
      "loss": 0.0007,
      "step": 3060
    },
    {
      "epoch": 5.6642066420664205,
      "grad_norm": 0.014031638391315937,
      "learning_rate": 9.573170731707317e-06,
      "loss": 0.0007,
      "step": 3070
    },
    {
      "epoch": 5.682656826568266,
      "grad_norm": 0.015066033229231834,
      "learning_rate": 9.532520325203252e-06,
      "loss": 0.1319,
      "step": 3080
    },
    {
      "epoch": 5.7011070110701105,
      "grad_norm": 0.017947029322385788,
      "learning_rate": 9.491869918699188e-06,
      "loss": 0.085,
      "step": 3090
    },
    {
      "epoch": 5.719557195571956,
      "grad_norm": 0.024357087910175323,
      "learning_rate": 9.451219512195122e-06,
      "loss": 0.0007,
      "step": 3100
    },
    {
      "epoch": 5.7380073800738005,
      "grad_norm": 0.02609538473188877,
      "learning_rate": 9.410569105691058e-06,
      "loss": 0.0008,
      "step": 3110
    },
    {
      "epoch": 5.756457564575646,
      "grad_norm": 0.010005384683609009,
      "learning_rate": 9.369918699186993e-06,
      "loss": 0.079,
      "step": 3120
    },
    {
      "epoch": 5.7749077490774905,
      "grad_norm": 0.012571939267218113,
      "learning_rate": 9.329268292682927e-06,
      "loss": 0.0006,
      "step": 3130
    },
    {
      "epoch": 5.793357933579336,
      "grad_norm": 0.007994637824594975,
      "learning_rate": 9.288617886178862e-06,
      "loss": 0.001,
      "step": 3140
    },
    {
      "epoch": 5.8118081180811805,
      "grad_norm": 0.03275281563401222,
      "learning_rate": 9.247967479674797e-06,
      "loss": 0.1641,
      "step": 3150
    },
    {
      "epoch": 5.830258302583026,
      "grad_norm": 0.01947631500661373,
      "learning_rate": 9.207317073170733e-06,
      "loss": 0.001,
      "step": 3160
    },
    {
      "epoch": 5.8487084870848705,
      "grad_norm": 0.016393976286053658,
      "learning_rate": 9.166666666666666e-06,
      "loss": 0.0009,
      "step": 3170
    },
    {
      "epoch": 5.867158671586716,
      "grad_norm": 0.01715436764061451,
      "learning_rate": 9.126016260162603e-06,
      "loss": 0.0008,
      "step": 3180
    },
    {
      "epoch": 5.885608856088561,
      "grad_norm": 0.015045661479234695,
      "learning_rate": 9.085365853658538e-06,
      "loss": 0.0006,
      "step": 3190
    },
    {
      "epoch": 5.904059040590406,
      "grad_norm": 0.04068911075592041,
      "learning_rate": 9.044715447154472e-06,
      "loss": 0.0006,
      "step": 3200
    },
    {
      "epoch": 5.922509225092251,
      "grad_norm": 0.010667635127902031,
      "learning_rate": 9.004065040650407e-06,
      "loss": 0.0534,
      "step": 3210
    },
    {
      "epoch": 5.940959409594096,
      "grad_norm": 0.009628127329051495,
      "learning_rate": 8.963414634146342e-06,
      "loss": 0.0004,
      "step": 3220
    },
    {
      "epoch": 5.959409594095941,
      "grad_norm": 0.009857387281954288,
      "learning_rate": 8.922764227642278e-06,
      "loss": 0.0012,
      "step": 3230
    },
    {
      "epoch": 5.977859778597786,
      "grad_norm": 0.010078341700136662,
      "learning_rate": 8.882113821138211e-06,
      "loss": 0.0005,
      "step": 3240
    },
    {
      "epoch": 5.996309963099631,
      "grad_norm": 0.008471671491861343,
      "learning_rate": 8.841463414634148e-06,
      "loss": 0.0004,
      "step": 3250
    },
    {
      "epoch": 6.0,
      "eval_accuracy": 0.966789667896679,
      "eval_f1": 0.9663551401869159,
      "eval_loss": 0.2284158319234848,
      "eval_precision": 0.9645522388059702,
      "eval_recall": 0.9681647940074907,
      "eval_runtime": 54.704,
      "eval_samples_per_second": 19.816,
      "eval_steps_per_second": 1.243,
      "step": 3252
    },
    {
      "epoch": 6.014760147601476,
      "grad_norm": 0.007520219776779413,
      "learning_rate": 8.800813008130082e-06,
      "loss": 0.0003,
      "step": 3260
    },
    {
      "epoch": 6.033210332103321,
      "grad_norm": 0.008230598643422127,
      "learning_rate": 8.760162601626017e-06,
      "loss": 0.0004,
      "step": 3270
    },
    {
      "epoch": 6.051660516605166,
      "grad_norm": 0.005383858922868967,
      "learning_rate": 8.719512195121952e-06,
      "loss": 0.0003,
      "step": 3280
    },
    {
      "epoch": 6.070110701107011,
      "grad_norm": 0.009072639979422092,
      "learning_rate": 8.678861788617887e-06,
      "loss": 0.0003,
      "step": 3290
    },
    {
      "epoch": 6.088560885608856,
      "grad_norm": 0.009099016897380352,
      "learning_rate": 8.638211382113821e-06,
      "loss": 0.0004,
      "step": 3300
    },
    {
      "epoch": 6.107011070110701,
      "grad_norm": 0.15623310208320618,
      "learning_rate": 8.597560975609756e-06,
      "loss": 0.0003,
      "step": 3310
    },
    {
      "epoch": 6.125461254612546,
      "grad_norm": 0.008997993543744087,
      "learning_rate": 8.556910569105693e-06,
      "loss": 0.0003,
      "step": 3320
    },
    {
      "epoch": 6.143911439114391,
      "grad_norm": 0.0047713713720440865,
      "learning_rate": 8.516260162601627e-06,
      "loss": 0.0003,
      "step": 3330
    },
    {
      "epoch": 6.162361623616236,
      "grad_norm": 0.006010842975229025,
      "learning_rate": 8.475609756097562e-06,
      "loss": 0.0052,
      "step": 3340
    },
    {
      "epoch": 6.180811808118081,
      "grad_norm": 0.006029469892382622,
      "learning_rate": 8.434959349593497e-06,
      "loss": 0.0005,
      "step": 3350
    },
    {
      "epoch": 6.199261992619927,
      "grad_norm": 0.0038303795736283064,
      "learning_rate": 8.394308943089432e-06,
      "loss": 0.0016,
      "step": 3360
    },
    {
      "epoch": 6.217712177121771,
      "grad_norm": 0.005448078736662865,
      "learning_rate": 8.353658536585366e-06,
      "loss": 0.0004,
      "step": 3370
    },
    {
      "epoch": 6.236162361623617,
      "grad_norm": 0.005202803295105696,
      "learning_rate": 8.313008130081301e-06,
      "loss": 0.0002,
      "step": 3380
    },
    {
      "epoch": 6.254612546125461,
      "grad_norm": 0.006949708331376314,
      "learning_rate": 8.272357723577238e-06,
      "loss": 0.006,
      "step": 3390
    },
    {
      "epoch": 6.273062730627307,
      "grad_norm": 0.005467634182423353,
      "learning_rate": 8.23170731707317e-06,
      "loss": 0.001,
      "step": 3400
    },
    {
      "epoch": 6.291512915129151,
      "grad_norm": 0.0041988929733633995,
      "learning_rate": 8.191056910569107e-06,
      "loss": 0.0002,
      "step": 3410
    },
    {
      "epoch": 6.3099630996309966,
      "grad_norm": 0.00414294982329011,
      "learning_rate": 8.150406504065042e-06,
      "loss": 0.0002,
      "step": 3420
    },
    {
      "epoch": 6.328413284132841,
      "grad_norm": 0.0040751248598098755,
      "learning_rate": 8.109756097560977e-06,
      "loss": 0.0002,
      "step": 3430
    },
    {
      "epoch": 6.3468634686346865,
      "grad_norm": 4.089688777923584,
      "learning_rate": 8.069105691056911e-06,
      "loss": 0.2908,
      "step": 3440
    },
    {
      "epoch": 6.365313653136531,
      "grad_norm": 0.0055015478283166885,
      "learning_rate": 8.028455284552846e-06,
      "loss": 0.0002,
      "step": 3450
    },
    {
      "epoch": 6.3837638376383765,
      "grad_norm": 0.008069195784628391,
      "learning_rate": 7.98780487804878e-06,
      "loss": 0.0844,
      "step": 3460
    },
    {
      "epoch": 6.402214022140221,
      "grad_norm": 0.010532557964324951,
      "learning_rate": 7.947154471544715e-06,
      "loss": 0.0005,
      "step": 3470
    },
    {
      "epoch": 6.4206642066420665,
      "grad_norm": 0.008565276861190796,
      "learning_rate": 7.90650406504065e-06,
      "loss": 0.0004,
      "step": 3480
    },
    {
      "epoch": 6.439114391143911,
      "grad_norm": 0.013668409548699856,
      "learning_rate": 7.865853658536587e-06,
      "loss": 0.0004,
      "step": 3490
    },
    {
      "epoch": 6.4575645756457565,
      "grad_norm": 108.54691314697266,
      "learning_rate": 7.82520325203252e-06,
      "loss": 0.0157,
      "step": 3500
    },
    {
      "epoch": 6.476014760147601,
      "grad_norm": 0.014828619547188282,
      "learning_rate": 7.784552845528456e-06,
      "loss": 0.0987,
      "step": 3510
    },
    {
      "epoch": 6.4944649446494465,
      "grad_norm": 0.009291148744523525,
      "learning_rate": 7.743902439024391e-06,
      "loss": 0.0006,
      "step": 3520
    },
    {
      "epoch": 6.512915129151292,
      "grad_norm": 0.02177053689956665,
      "learning_rate": 7.703252032520326e-06,
      "loss": 0.0747,
      "step": 3530
    },
    {
      "epoch": 6.531365313653136,
      "grad_norm": 0.009995128028094769,
      "learning_rate": 7.66260162601626e-06,
      "loss": 0.0005,
      "step": 3540
    },
    {
      "epoch": 6.549815498154982,
      "grad_norm": 0.00891880039125681,
      "learning_rate": 7.621951219512196e-06,
      "loss": 0.0005,
      "step": 3550
    },
    {
      "epoch": 6.568265682656826,
      "grad_norm": 0.010180546902120113,
      "learning_rate": 7.5813008130081316e-06,
      "loss": 0.0004,
      "step": 3560
    },
    {
      "epoch": 6.586715867158672,
      "grad_norm": 0.009459234774112701,
      "learning_rate": 7.5406504065040654e-06,
      "loss": 0.0004,
      "step": 3570
    },
    {
      "epoch": 6.605166051660516,
      "grad_norm": 0.008564491756260395,
      "learning_rate": 7.500000000000001e-06,
      "loss": 0.0003,
      "step": 3580
    },
    {
      "epoch": 6.623616236162362,
      "grad_norm": 0.00567926000803709,
      "learning_rate": 7.459349593495936e-06,
      "loss": 0.0003,
      "step": 3590
    },
    {
      "epoch": 6.642066420664206,
      "grad_norm": 0.007786845788359642,
      "learning_rate": 7.41869918699187e-06,
      "loss": 0.0003,
      "step": 3600
    },
    {
      "epoch": 6.660516605166052,
      "grad_norm": 0.007092857267707586,
      "learning_rate": 7.378048780487805e-06,
      "loss": 0.0003,
      "step": 3610
    },
    {
      "epoch": 6.678966789667896,
      "grad_norm": 0.00563403032720089,
      "learning_rate": 7.337398373983741e-06,
      "loss": 0.0003,
      "step": 3620
    },
    {
      "epoch": 6.697416974169742,
      "grad_norm": 0.00976848229765892,
      "learning_rate": 7.296747967479675e-06,
      "loss": 0.0134,
      "step": 3630
    },
    {
      "epoch": 6.715867158671586,
      "grad_norm": 0.005455783102661371,
      "learning_rate": 7.25609756097561e-06,
      "loss": 0.0003,
      "step": 3640
    },
    {
      "epoch": 6.734317343173432,
      "grad_norm": 0.007207727059721947,
      "learning_rate": 7.215447154471545e-06,
      "loss": 0.0859,
      "step": 3650
    },
    {
      "epoch": 6.752767527675276,
      "grad_norm": 0.011984763666987419,
      "learning_rate": 7.174796747967481e-06,
      "loss": 0.0003,
      "step": 3660
    },
    {
      "epoch": 6.771217712177122,
      "grad_norm": 0.00722339004278183,
      "learning_rate": 7.1341463414634146e-06,
      "loss": 0.0003,
      "step": 3670
    },
    {
      "epoch": 6.789667896678967,
      "grad_norm": 0.006009739823639393,
      "learning_rate": 7.09349593495935e-06,
      "loss": 0.0003,
      "step": 3680
    },
    {
      "epoch": 6.808118081180812,
      "grad_norm": 0.12036023288965225,
      "learning_rate": 7.052845528455286e-06,
      "loss": 0.0929,
      "step": 3690
    },
    {
      "epoch": 6.826568265682657,
      "grad_norm": 0.009958956390619278,
      "learning_rate": 7.01219512195122e-06,
      "loss": 0.0003,
      "step": 3700
    },
    {
      "epoch": 6.845018450184502,
      "grad_norm": 0.00784386321902275,
      "learning_rate": 6.971544715447155e-06,
      "loss": 0.0349,
      "step": 3710
    },
    {
      "epoch": 6.863468634686347,
      "grad_norm": 0.014620929956436157,
      "learning_rate": 6.93089430894309e-06,
      "loss": 0.0004,
      "step": 3720
    },
    {
      "epoch": 6.881918819188192,
      "grad_norm": 0.007814665324985981,
      "learning_rate": 6.890243902439025e-06,
      "loss": 0.0003,
      "step": 3730
    },
    {
      "epoch": 6.900369003690037,
      "grad_norm": 0.006984157487750053,
      "learning_rate": 6.8495934959349595e-06,
      "loss": 0.0004,
      "step": 3740
    },
    {
      "epoch": 6.918819188191882,
      "grad_norm": 0.006928198505192995,
      "learning_rate": 6.808943089430895e-06,
      "loss": 0.0004,
      "step": 3750
    },
    {
      "epoch": 6.937269372693727,
      "grad_norm": 0.0081993592903018,
      "learning_rate": 6.768292682926831e-06,
      "loss": 0.0004,
      "step": 3760
    },
    {
      "epoch": 6.955719557195572,
      "grad_norm": 0.0064469510689377785,
      "learning_rate": 6.7276422764227645e-06,
      "loss": 0.0592,
      "step": 3770
    },
    {
      "epoch": 6.974169741697417,
      "grad_norm": 0.007447575684636831,
      "learning_rate": 6.6869918699187e-06,
      "loss": 0.0003,
      "step": 3780
    },
    {
      "epoch": 6.992619926199262,
      "grad_norm": 0.025975581258535385,
      "learning_rate": 6.646341463414635e-06,
      "loss": 0.0962,
      "step": 3790
    },
    {
      "epoch": 7.0,
      "eval_accuracy": 0.9714022140221402,
      "eval_f1": 0.9708920187793427,
      "eval_loss": 0.17706093192100525,
      "eval_precision": 0.9736346516007532,
      "eval_recall": 0.9681647940074907,
      "eval_runtime": 53.9833,
      "eval_samples_per_second": 20.08,
      "eval_steps_per_second": 1.26,
      "step": 3794
    },
    {
      "epoch": 7.011070110701107,
      "grad_norm": 0.01766619086265564,
      "learning_rate": 6.60569105691057e-06,
      "loss": 0.0012,
      "step": 3800
    },
    {
      "epoch": 7.029520295202952,
      "grad_norm": 0.011769936420023441,
      "learning_rate": 6.565040650406504e-06,
      "loss": 0.0004,
      "step": 3810
    },
    {
      "epoch": 7.047970479704797,
      "grad_norm": 0.008464880287647247,
      "learning_rate": 6.52439024390244e-06,
      "loss": 0.0005,
      "step": 3820
    },
    {
      "epoch": 7.0664206642066425,
      "grad_norm": 0.012654009275138378,
      "learning_rate": 6.483739837398374e-06,
      "loss": 0.0004,
      "step": 3830
    },
    {
      "epoch": 7.084870848708487,
      "grad_norm": 0.009040876291692257,
      "learning_rate": 6.4430894308943094e-06,
      "loss": 0.0543,
      "step": 3840
    },
    {
      "epoch": 7.1033210332103325,
      "grad_norm": 0.009602717123925686,
      "learning_rate": 6.402439024390244e-06,
      "loss": 0.1016,
      "step": 3850
    },
    {
      "epoch": 7.121771217712177,
      "grad_norm": 0.025668678805232048,
      "learning_rate": 6.36178861788618e-06,
      "loss": 0.0004,
      "step": 3860
    },
    {
      "epoch": 7.1402214022140225,
      "grad_norm": 0.008670873008668423,
      "learning_rate": 6.321138211382114e-06,
      "loss": 0.0081,
      "step": 3870
    },
    {
      "epoch": 7.158671586715867,
      "grad_norm": 0.012239494360983372,
      "learning_rate": 6.280487804878049e-06,
      "loss": 0.0005,
      "step": 3880
    },
    {
      "epoch": 7.177121771217712,
      "grad_norm": 0.011602608487010002,
      "learning_rate": 6.239837398373985e-06,
      "loss": 0.0005,
      "step": 3890
    },
    {
      "epoch": 7.195571955719557,
      "grad_norm": 0.008720600977540016,
      "learning_rate": 6.199186991869919e-06,
      "loss": 0.0004,
      "step": 3900
    },
    {
      "epoch": 7.214022140221402,
      "grad_norm": 0.009708746336400509,
      "learning_rate": 6.158536585365854e-06,
      "loss": 0.0003,
      "step": 3910
    },
    {
      "epoch": 7.232472324723247,
      "grad_norm": 0.009403160773217678,
      "learning_rate": 6.117886178861789e-06,
      "loss": 0.0003,
      "step": 3920
    },
    {
      "epoch": 7.250922509225092,
      "grad_norm": 0.007796939928084612,
      "learning_rate": 6.077235772357724e-06,
      "loss": 0.0264,
      "step": 3930
    },
    {
      "epoch": 7.269372693726937,
      "grad_norm": 0.00665581040084362,
      "learning_rate": 6.0365853658536585e-06,
      "loss": 0.0003,
      "step": 3940
    },
    {
      "epoch": 7.287822878228782,
      "grad_norm": 0.024884402751922607,
      "learning_rate": 5.995934959349594e-06,
      "loss": 0.0003,
      "step": 3950
    },
    {
      "epoch": 7.306273062730627,
      "grad_norm": 0.010905355215072632,
      "learning_rate": 5.95528455284553e-06,
      "loss": 0.0003,
      "step": 3960
    },
    {
      "epoch": 7.324723247232472,
      "grad_norm": 0.005571721121668816,
      "learning_rate": 5.914634146341464e-06,
      "loss": 0.0003,
      "step": 3970
    },
    {
      "epoch": 7.343173431734318,
      "grad_norm": 0.006296244915574789,
      "learning_rate": 5.873983739837399e-06,
      "loss": 0.0003,
      "step": 3980
    },
    {
      "epoch": 7.361623616236162,
      "grad_norm": 0.052488405257463455,
      "learning_rate": 5.833333333333334e-06,
      "loss": 0.0003,
      "step": 3990
    },
    {
      "epoch": 7.380073800738008,
      "grad_norm": 0.003874011104926467,
      "learning_rate": 5.792682926829269e-06,
      "loss": 0.0002,
      "step": 4000
    },
    {
      "epoch": 7.398523985239852,
      "grad_norm": 0.005483183078467846,
      "learning_rate": 5.7520325203252034e-06,
      "loss": 0.0002,
      "step": 4010
    },
    {
      "epoch": 7.416974169741698,
      "grad_norm": 0.028192806988954544,
      "learning_rate": 5.711382113821139e-06,
      "loss": 0.0003,
      "step": 4020
    },
    {
      "epoch": 7.435424354243542,
      "grad_norm": 0.006092531140893698,
      "learning_rate": 5.670731707317073e-06,
      "loss": 0.0002,
      "step": 4030
    },
    {
      "epoch": 7.453874538745388,
      "grad_norm": 0.004346298519521952,
      "learning_rate": 5.6300813008130085e-06,
      "loss": 0.0002,
      "step": 4040
    },
    {
      "epoch": 7.472324723247232,
      "grad_norm": 0.0038129182066768408,
      "learning_rate": 5.589430894308944e-06,
      "loss": 0.0002,
      "step": 4050
    },
    {
      "epoch": 7.490774907749078,
      "grad_norm": 0.003967374563217163,
      "learning_rate": 5.548780487804879e-06,
      "loss": 0.0002,
      "step": 4060
    },
    {
      "epoch": 7.509225092250922,
      "grad_norm": 0.00627779820933938,
      "learning_rate": 5.508130081300814e-06,
      "loss": 0.1002,
      "step": 4070
    },
    {
      "epoch": 7.527675276752768,
      "grad_norm": 0.008496380411088467,
      "learning_rate": 5.467479674796748e-06,
      "loss": 0.0364,
      "step": 4080
    },
    {
      "epoch": 7.546125461254612,
      "grad_norm": 0.014082346111536026,
      "learning_rate": 5.426829268292684e-06,
      "loss": 0.1775,
      "step": 4090
    },
    {
      "epoch": 7.564575645756458,
      "grad_norm": 0.28857582807540894,
      "learning_rate": 5.386178861788618e-06,
      "loss": 0.0029,
      "step": 4100
    },
    {
      "epoch": 7.583025830258302,
      "grad_norm": 0.08894560486078262,
      "learning_rate": 5.345528455284553e-06,
      "loss": 0.0008,
      "step": 4110
    },
    {
      "epoch": 7.601476014760148,
      "grad_norm": 0.008145516738295555,
      "learning_rate": 5.304878048780488e-06,
      "loss": 0.0005,
      "step": 4120
    },
    {
      "epoch": 7.619926199261993,
      "grad_norm": 0.24613209068775177,
      "learning_rate": 5.264227642276423e-06,
      "loss": 0.0005,
      "step": 4130
    },
    {
      "epoch": 7.638376383763838,
      "grad_norm": 0.045201875269412994,
      "learning_rate": 5.223577235772358e-06,
      "loss": 0.0004,
      "step": 4140
    },
    {
      "epoch": 7.656826568265682,
      "grad_norm": 0.020961016416549683,
      "learning_rate": 5.182926829268293e-06,
      "loss": 0.0005,
      "step": 4150
    },
    {
      "epoch": 7.675276752767528,
      "grad_norm": 0.007874876260757446,
      "learning_rate": 5.142276422764229e-06,
      "loss": 0.0004,
      "step": 4160
    },
    {
      "epoch": 7.693726937269373,
      "grad_norm": 0.009890733286738396,
      "learning_rate": 5.101626016260163e-06,
      "loss": 0.0003,
      "step": 4170
    },
    {
      "epoch": 7.712177121771218,
      "grad_norm": 0.0073625473305583,
      "learning_rate": 5.060975609756098e-06,
      "loss": 0.0004,
      "step": 4180
    },
    {
      "epoch": 7.730627306273063,
      "grad_norm": 0.01290129404515028,
      "learning_rate": 5.020325203252033e-06,
      "loss": 0.0003,
      "step": 4190
    },
    {
      "epoch": 7.749077490774908,
      "grad_norm": 0.009389232844114304,
      "learning_rate": 4.979674796747968e-06,
      "loss": 0.0003,
      "step": 4200
    },
    {
      "epoch": 7.767527675276753,
      "grad_norm": 0.006532302126288414,
      "learning_rate": 4.9390243902439025e-06,
      "loss": 0.0003,
      "step": 4210
    },
    {
      "epoch": 7.785977859778598,
      "grad_norm": 0.00750455679371953,
      "learning_rate": 4.898373983739837e-06,
      "loss": 0.0003,
      "step": 4220
    },
    {
      "epoch": 7.804428044280443,
      "grad_norm": 0.005900105461478233,
      "learning_rate": 4.857723577235773e-06,
      "loss": 0.0003,
      "step": 4230
    },
    {
      "epoch": 7.822878228782288,
      "grad_norm": 0.00444071227684617,
      "learning_rate": 4.817073170731708e-06,
      "loss": 0.0003,
      "step": 4240
    },
    {
      "epoch": 7.841328413284133,
      "grad_norm": 0.005041477736085653,
      "learning_rate": 4.776422764227643e-06,
      "loss": 0.0003,
      "step": 4250
    },
    {
      "epoch": 7.8597785977859775,
      "grad_norm": 0.00623696343973279,
      "learning_rate": 4.735772357723578e-06,
      "loss": 0.0003,
      "step": 4260
    },
    {
      "epoch": 7.878228782287823,
      "grad_norm": 0.0051438817754387856,
      "learning_rate": 4.695121951219513e-06,
      "loss": 0.0615,
      "step": 4270
    },
    {
      "epoch": 7.8966789667896675,
      "grad_norm": 0.009021712467074394,
      "learning_rate": 4.654471544715447e-06,
      "loss": 0.0003,
      "step": 4280
    },
    {
      "epoch": 7.915129151291513,
      "grad_norm": 0.006667012348771095,
      "learning_rate": 4.613821138211382e-06,
      "loss": 0.1014,
      "step": 4290
    },
    {
      "epoch": 7.9335793357933575,
      "grad_norm": 0.026189584285020828,
      "learning_rate": 4.573170731707318e-06,
      "loss": 0.0008,
      "step": 4300
    },
    {
      "epoch": 7.952029520295203,
      "grad_norm": 0.0094147939234972,
      "learning_rate": 4.5325203252032525e-06,
      "loss": 0.0003,
      "step": 4310
    },
    {
      "epoch": 7.970479704797048,
      "grad_norm": 0.008884645998477936,
      "learning_rate": 4.491869918699187e-06,
      "loss": 0.0004,
      "step": 4320
    },
    {
      "epoch": 7.988929889298893,
      "grad_norm": 0.00615200400352478,
      "learning_rate": 4.451219512195122e-06,
      "loss": 0.0003,
      "step": 4330
    },
    {
      "epoch": 8.0,
      "eval_accuracy": 0.9686346863468634,
      "eval_f1": 0.9682835820895523,
      "eval_loss": 0.23687304556369781,
      "eval_precision": 0.9646840148698885,
      "eval_recall": 0.9719101123595506,
      "eval_runtime": 51.7113,
      "eval_samples_per_second": 20.963,
      "eval_steps_per_second": 1.315,
      "step": 4336
    },
    {
      "epoch": 8.007380073800737,
      "grad_norm": 0.008305331692099571,
      "learning_rate": 4.410569105691057e-06,
      "loss": 0.0286,
      "step": 4340
    },
    {
      "epoch": 8.025830258302584,
      "grad_norm": 0.006246690638363361,
      "learning_rate": 4.369918699186992e-06,
      "loss": 0.0003,
      "step": 4350
    },
    {
      "epoch": 8.044280442804428,
      "grad_norm": 0.0068855625577270985,
      "learning_rate": 4.329268292682927e-06,
      "loss": 0.0003,
      "step": 4360
    },
    {
      "epoch": 8.062730627306273,
      "grad_norm": 0.007796368096023798,
      "learning_rate": 4.288617886178862e-06,
      "loss": 0.0003,
      "step": 4370
    },
    {
      "epoch": 8.081180811808117,
      "grad_norm": 0.016771048307418823,
      "learning_rate": 4.247967479674797e-06,
      "loss": 0.0005,
      "step": 4380
    },
    {
      "epoch": 8.099630996309964,
      "grad_norm": 0.0068199816159904,
      "learning_rate": 4.207317073170732e-06,
      "loss": 0.0003,
      "step": 4390
    },
    {
      "epoch": 8.118081180811808,
      "grad_norm": 0.007829892449080944,
      "learning_rate": 4.166666666666667e-06,
      "loss": 0.0003,
      "step": 4400
    },
    {
      "epoch": 8.136531365313653,
      "grad_norm": 0.006034753285348415,
      "learning_rate": 4.126016260162602e-06,
      "loss": 0.0002,
      "step": 4410
    },
    {
      "epoch": 8.154981549815497,
      "grad_norm": 0.006216737907379866,
      "learning_rate": 4.085365853658536e-06,
      "loss": 0.0003,
      "step": 4420
    },
    {
      "epoch": 8.173431734317344,
      "grad_norm": 0.005210587754845619,
      "learning_rate": 4.044715447154472e-06,
      "loss": 0.0875,
      "step": 4430
    },
    {
      "epoch": 8.191881918819188,
      "grad_norm": 0.005358359310775995,
      "learning_rate": 4.004065040650407e-06,
      "loss": 0.0002,
      "step": 4440
    },
    {
      "epoch": 8.210332103321033,
      "grad_norm": 0.004295751918107271,
      "learning_rate": 3.963414634146342e-06,
      "loss": 0.0006,
      "step": 4450
    },
    {
      "epoch": 8.228782287822877,
      "grad_norm": 2.564018487930298,
      "learning_rate": 3.922764227642277e-06,
      "loss": 0.0005,
      "step": 4460
    },
    {
      "epoch": 8.247232472324724,
      "grad_norm": 0.005549343768507242,
      "learning_rate": 3.882113821138212e-06,
      "loss": 0.0003,
      "step": 4470
    },
    {
      "epoch": 8.265682656826568,
      "grad_norm": 0.0038247182965278625,
      "learning_rate": 3.8414634146341465e-06,
      "loss": 0.0003,
      "step": 4480
    },
    {
      "epoch": 8.284132841328413,
      "grad_norm": 0.007086277473717928,
      "learning_rate": 3.8008130081300817e-06,
      "loss": 0.0002,
      "step": 4490
    },
    {
      "epoch": 8.302583025830259,
      "grad_norm": 0.00542485760524869,
      "learning_rate": 3.760162601626017e-06,
      "loss": 0.0002,
      "step": 4500
    },
    {
      "epoch": 8.321033210332104,
      "grad_norm": 0.0042741321958601475,
      "learning_rate": 3.7195121951219516e-06,
      "loss": 0.0002,
      "step": 4510
    },
    {
      "epoch": 8.339483394833948,
      "grad_norm": 0.007002950180321932,
      "learning_rate": 3.6788617886178863e-06,
      "loss": 0.0002,
      "step": 4520
    },
    {
      "epoch": 8.357933579335793,
      "grad_norm": 0.00571675319224596,
      "learning_rate": 3.6382113821138215e-06,
      "loss": 0.0002,
      "step": 4530
    },
    {
      "epoch": 8.376383763837639,
      "grad_norm": 0.004431506618857384,
      "learning_rate": 3.5975609756097562e-06,
      "loss": 0.0002,
      "step": 4540
    },
    {
      "epoch": 8.394833948339484,
      "grad_norm": 0.004062026273459196,
      "learning_rate": 3.5569105691056914e-06,
      "loss": 0.0002,
      "step": 4550
    },
    {
      "epoch": 8.413284132841328,
      "grad_norm": 0.00478143198415637,
      "learning_rate": 3.516260162601626e-06,
      "loss": 0.0002,
      "step": 4560
    },
    {
      "epoch": 8.431734317343173,
      "grad_norm": 0.005338719580322504,
      "learning_rate": 3.475609756097561e-06,
      "loss": 0.0002,
      "step": 4570
    },
    {
      "epoch": 8.450184501845019,
      "grad_norm": 0.0043670404702425,
      "learning_rate": 3.4349593495934965e-06,
      "loss": 0.0002,
      "step": 4580
    },
    {
      "epoch": 8.468634686346864,
      "grad_norm": 0.003354250220581889,
      "learning_rate": 3.394308943089431e-06,
      "loss": 0.0002,
      "step": 4590
    },
    {
      "epoch": 8.487084870848708,
      "grad_norm": 0.003936729859560728,
      "learning_rate": 3.3536585365853664e-06,
      "loss": 0.0002,
      "step": 4600
    },
    {
      "epoch": 8.505535055350553,
      "grad_norm": 0.00421642092987895,
      "learning_rate": 3.313008130081301e-06,
      "loss": 0.0026,
      "step": 4610
    },
    {
      "epoch": 8.523985239852399,
      "grad_norm": 0.0036806506104767323,
      "learning_rate": 3.272357723577236e-06,
      "loss": 0.0002,
      "step": 4620
    },
    {
      "epoch": 8.542435424354244,
      "grad_norm": 0.005413874983787537,
      "learning_rate": 3.231707317073171e-06,
      "loss": 0.0002,
      "step": 4630
    },
    {
      "epoch": 8.560885608856088,
      "grad_norm": 0.007322368212044239,
      "learning_rate": 3.1910569105691058e-06,
      "loss": 0.1025,
      "step": 4640
    },
    {
      "epoch": 8.579335793357934,
      "grad_norm": 0.004331518430262804,
      "learning_rate": 3.150406504065041e-06,
      "loss": 0.0002,
      "step": 4650
    },
    {
      "epoch": 8.597785977859779,
      "grad_norm": 0.004401362966746092,
      "learning_rate": 3.1097560975609757e-06,
      "loss": 0.0002,
      "step": 4660
    },
    {
      "epoch": 8.616236162361623,
      "grad_norm": 0.027949245646595955,
      "learning_rate": 3.0691056910569104e-06,
      "loss": 0.0003,
      "step": 4670
    },
    {
      "epoch": 8.634686346863468,
      "grad_norm": 0.007964776828885078,
      "learning_rate": 3.028455284552846e-06,
      "loss": 0.0003,
      "step": 4680
    },
    {
      "epoch": 8.653136531365314,
      "grad_norm": 0.00477459654211998,
      "learning_rate": 2.9878048780487808e-06,
      "loss": 0.0002,
      "step": 4690
    },
    {
      "epoch": 8.671586715867159,
      "grad_norm": 0.015770891681313515,
      "learning_rate": 2.947154471544716e-06,
      "loss": 0.1059,
      "step": 4700
    },
    {
      "epoch": 8.690036900369003,
      "grad_norm": 0.005496690049767494,
      "learning_rate": 2.9065040650406507e-06,
      "loss": 0.0003,
      "step": 4710
    },
    {
      "epoch": 8.708487084870848,
      "grad_norm": 0.007938714697957039,
      "learning_rate": 2.8658536585365854e-06,
      "loss": 0.0003,
      "step": 4720
    },
    {
      "epoch": 8.726937269372694,
      "grad_norm": 0.009085195139050484,
      "learning_rate": 2.8252032520325206e-06,
      "loss": 0.0003,
      "step": 4730
    },
    {
      "epoch": 8.745387453874539,
      "grad_norm": 0.0037721297703683376,
      "learning_rate": 2.7845528455284553e-06,
      "loss": 0.1049,
      "step": 4740
    },
    {
      "epoch": 8.763837638376383,
      "grad_norm": 0.0046789743937551975,
      "learning_rate": 2.7439024390243905e-06,
      "loss": 0.0003,
      "step": 4750
    },
    {
      "epoch": 8.782287822878228,
      "grad_norm": 0.028253531083464622,
      "learning_rate": 2.7032520325203252e-06,
      "loss": 0.0003,
      "step": 4760
    },
    {
      "epoch": 8.800738007380074,
      "grad_norm": 0.009016442112624645,
      "learning_rate": 2.66260162601626e-06,
      "loss": 0.0003,
      "step": 4770
    },
    {
      "epoch": 8.819188191881919,
      "grad_norm": 0.0359254814684391,
      "learning_rate": 2.6219512195121956e-06,
      "loss": 0.0003,
      "step": 4780
    },
    {
      "epoch": 8.837638376383763,
      "grad_norm": 0.004341773688793182,
      "learning_rate": 2.5813008130081303e-06,
      "loss": 0.0036,
      "step": 4790
    },
    {
      "epoch": 8.85608856088561,
      "grad_norm": 0.014118912629783154,
      "learning_rate": 2.5406504065040655e-06,
      "loss": 0.0003,
      "step": 4800
    },
    {
      "epoch": 8.874538745387454,
      "grad_norm": 0.004963293671607971,
      "learning_rate": 2.5e-06,
      "loss": 0.0003,
      "step": 4810
    },
    {
      "epoch": 8.892988929889299,
      "grad_norm": 0.0053186859004199505,
      "learning_rate": 2.4593495934959354e-06,
      "loss": 0.0002,
      "step": 4820
    },
    {
      "epoch": 8.911439114391143,
      "grad_norm": 0.003976750187575817,
      "learning_rate": 2.41869918699187e-06,
      "loss": 0.0002,
      "step": 4830
    },
    {
      "epoch": 8.92988929889299,
      "grad_norm": 0.005180181469768286,
      "learning_rate": 2.378048780487805e-06,
      "loss": 0.0002,
      "step": 4840
    },
    {
      "epoch": 8.948339483394834,
      "grad_norm": 0.0066690267994999886,
      "learning_rate": 2.33739837398374e-06,
      "loss": 0.0002,
      "step": 4850
    },
    {
      "epoch": 8.966789667896679,
      "grad_norm": 0.007578693795949221,
      "learning_rate": 2.296747967479675e-06,
      "loss": 0.0002,
      "step": 4860
    },
    {
      "epoch": 8.985239852398523,
      "grad_norm": 0.007683157920837402,
      "learning_rate": 2.25609756097561e-06,
      "loss": 0.0763,
      "step": 4870
    },
    {
      "epoch": 9.0,
      "eval_accuracy": 0.9714022140221402,
      "eval_f1": 0.9708372530573847,
      "eval_loss": 0.21400035917758942,
      "eval_precision": 0.9754253308128544,
      "eval_recall": 0.9662921348314607,
      "eval_runtime": 51.7297,
      "eval_samples_per_second": 20.955,
      "eval_steps_per_second": 1.315,
      "step": 4878
    },
    {
      "epoch": 9.00369003690037,
      "grad_norm": 0.005392983555793762,
      "learning_rate": 2.215447154471545e-06,
      "loss": 0.0003,
      "step": 4880
    },
    {
      "epoch": 9.022140221402214,
      "grad_norm": 0.003888051025569439,
      "learning_rate": 2.17479674796748e-06,
      "loss": 0.0002,
      "step": 4890
    },
    {
      "epoch": 9.040590405904059,
      "grad_norm": 0.005250315181910992,
      "learning_rate": 2.1341463414634146e-06,
      "loss": 0.0002,
      "step": 4900
    },
    {
      "epoch": 9.059040590405903,
      "grad_norm": 0.013148630037903786,
      "learning_rate": 2.0934959349593497e-06,
      "loss": 0.0002,
      "step": 4910
    },
    {
      "epoch": 9.07749077490775,
      "grad_norm": 0.005173043347895145,
      "learning_rate": 2.052845528455285e-06,
      "loss": 0.0005,
      "step": 4920
    },
    {
      "epoch": 9.095940959409594,
      "grad_norm": 0.004078742116689682,
      "learning_rate": 2.0121951219512197e-06,
      "loss": 0.0002,
      "step": 4930
    },
    {
      "epoch": 9.114391143911439,
      "grad_norm": 0.007535440381616354,
      "learning_rate": 1.9715447154471544e-06,
      "loss": 0.0002,
      "step": 4940
    },
    {
      "epoch": 9.132841328413285,
      "grad_norm": 0.006829661782830954,
      "learning_rate": 1.9308943089430896e-06,
      "loss": 0.0986,
      "step": 4950
    },
    {
      "epoch": 9.15129151291513,
      "grad_norm": 0.007270906586199999,
      "learning_rate": 1.8902439024390245e-06,
      "loss": 0.0002,
      "step": 4960
    },
    {
      "epoch": 9.169741697416974,
      "grad_norm": 0.006436340976506472,
      "learning_rate": 1.8495934959349595e-06,
      "loss": 0.0002,
      "step": 4970
    },
    {
      "epoch": 9.188191881918819,
      "grad_norm": 0.005766015034168959,
      "learning_rate": 1.8089430894308946e-06,
      "loss": 0.0002,
      "step": 4980
    },
    {
      "epoch": 9.206642066420665,
      "grad_norm": 0.008768351748585701,
      "learning_rate": 1.7682926829268294e-06,
      "loss": 0.0002,
      "step": 4990
    },
    {
      "epoch": 9.22509225092251,
      "grad_norm": 0.003120346227660775,
      "learning_rate": 1.7276422764227643e-06,
      "loss": 0.0007,
      "step": 5000
    },
    {
      "epoch": 9.243542435424354,
      "grad_norm": 0.005742421839386225,
      "learning_rate": 1.6869918699186993e-06,
      "loss": 0.0002,
      "step": 5010
    },
    {
      "epoch": 9.261992619926199,
      "grad_norm": 6.467769145965576,
      "learning_rate": 1.6463414634146345e-06,
      "loss": 0.0689,
      "step": 5020
    },
    {
      "epoch": 9.280442804428045,
      "grad_norm": 0.0044585708528757095,
      "learning_rate": 1.6056910569105694e-06,
      "loss": 0.0002,
      "step": 5030
    },
    {
      "epoch": 9.29889298892989,
      "grad_norm": 0.006484685931354761,
      "learning_rate": 1.5650406504065042e-06,
      "loss": 0.1799,
      "step": 5040
    },
    {
      "epoch": 9.317343173431734,
      "grad_norm": 0.024763647466897964,
      "learning_rate": 1.5243902439024391e-06,
      "loss": 0.0004,
      "step": 5050
    },
    {
      "epoch": 9.335793357933579,
      "grad_norm": 0.008389498107135296,
      "learning_rate": 1.483739837398374e-06,
      "loss": 0.0004,
      "step": 5060
    },
    {
      "epoch": 9.354243542435425,
      "grad_norm": 0.012540300376713276,
      "learning_rate": 1.4430894308943092e-06,
      "loss": 0.0005,
      "step": 5070
    },
    {
      "epoch": 9.37269372693727,
      "grad_norm": 0.005227524321526289,
      "learning_rate": 1.4024390243902442e-06,
      "loss": 0.0003,
      "step": 5080
    },
    {
      "epoch": 9.391143911439114,
      "grad_norm": 0.005190147552639246,
      "learning_rate": 1.361788617886179e-06,
      "loss": 0.0003,
      "step": 5090
    },
    {
      "epoch": 9.40959409594096,
      "grad_norm": 0.004168670158833265,
      "learning_rate": 1.3211382113821139e-06,
      "loss": 0.0003,
      "step": 5100
    },
    {
      "epoch": 9.428044280442805,
      "grad_norm": 0.009221001528203487,
      "learning_rate": 1.2804878048780488e-06,
      "loss": 0.0005,
      "step": 5110
    },
    {
      "epoch": 9.44649446494465,
      "grad_norm": 0.005992997903376818,
      "learning_rate": 1.2398373983739838e-06,
      "loss": 0.0003,
      "step": 5120
    },
    {
      "epoch": 9.464944649446494,
      "grad_norm": 0.008920296095311642,
      "learning_rate": 1.1991869918699187e-06,
      "loss": 0.0003,
      "step": 5130
    },
    {
      "epoch": 9.48339483394834,
      "grad_norm": 0.003542209044098854,
      "learning_rate": 1.158536585365854e-06,
      "loss": 0.0014,
      "step": 5140
    },
    {
      "epoch": 9.501845018450185,
      "grad_norm": 0.008030910976231098,
      "learning_rate": 1.1178861788617887e-06,
      "loss": 0.0003,
      "step": 5150
    },
    {
      "epoch": 9.52029520295203,
      "grad_norm": 0.006378370802849531,
      "learning_rate": 1.0772357723577236e-06,
      "loss": 0.063,
      "step": 5160
    },
    {
      "epoch": 9.538745387453874,
      "grad_norm": 0.016820160672068596,
      "learning_rate": 1.0365853658536586e-06,
      "loss": 0.0003,
      "step": 5170
    },
    {
      "epoch": 9.55719557195572,
      "grad_norm": 0.0036714859306812286,
      "learning_rate": 9.959349593495935e-07,
      "loss": 0.0003,
      "step": 5180
    },
    {
      "epoch": 9.575645756457565,
      "grad_norm": 0.009111960418522358,
      "learning_rate": 9.552845528455287e-07,
      "loss": 0.0003,
      "step": 5190
    },
    {
      "epoch": 9.59409594095941,
      "grad_norm": 0.007220650557428598,
      "learning_rate": 9.146341463414634e-07,
      "loss": 0.0005,
      "step": 5200
    },
    {
      "epoch": 9.612546125461254,
      "grad_norm": 0.016626538708806038,
      "learning_rate": 8.739837398373985e-07,
      "loss": 0.0003,
      "step": 5210
    },
    {
      "epoch": 9.6309963099631,
      "grad_norm": 0.007374123204499483,
      "learning_rate": 8.333333333333333e-07,
      "loss": 0.0003,
      "step": 5220
    },
    {
      "epoch": 9.649446494464945,
      "grad_norm": 0.006143673323094845,
      "learning_rate": 7.926829268292684e-07,
      "loss": 0.0003,
      "step": 5230
    },
    {
      "epoch": 9.66789667896679,
      "grad_norm": 0.018704796209931374,
      "learning_rate": 7.520325203252033e-07,
      "loss": 0.0003,
      "step": 5240
    },
    {
      "epoch": 9.686346863468636,
      "grad_norm": 0.006444479338824749,
      "learning_rate": 7.113821138211383e-07,
      "loss": 0.0003,
      "step": 5250
    },
    {
      "epoch": 9.70479704797048,
      "grad_norm": 0.006587930489331484,
      "learning_rate": 6.707317073170733e-07,
      "loss": 0.0003,
      "step": 5260
    },
    {
      "epoch": 9.723247232472325,
      "grad_norm": 0.005496221128851175,
      "learning_rate": 6.300813008130081e-07,
      "loss": 0.0003,
      "step": 5270
    },
    {
      "epoch": 9.74169741697417,
      "grad_norm": 0.006573851220309734,
      "learning_rate": 5.894308943089432e-07,
      "loss": 0.0004,
      "step": 5280
    },
    {
      "epoch": 9.760147601476016,
      "grad_norm": 0.007062075659632683,
      "learning_rate": 5.487804878048781e-07,
      "loss": 0.0003,
      "step": 5290
    },
    {
      "epoch": 9.77859778597786,
      "grad_norm": 0.009666337631642818,
      "learning_rate": 5.081300813008131e-07,
      "loss": 0.0003,
      "step": 5300
    },
    {
      "epoch": 9.797047970479705,
      "grad_norm": 0.005544922314584255,
      "learning_rate": 4.6747967479674797e-07,
      "loss": 0.0003,
      "step": 5310
    },
    {
      "epoch": 9.81549815498155,
      "grad_norm": 0.010419665835797787,
      "learning_rate": 4.26829268292683e-07,
      "loss": 0.0003,
      "step": 5320
    },
    {
      "epoch": 9.833948339483396,
      "grad_norm": 0.021195828914642334,
      "learning_rate": 3.8617886178861793e-07,
      "loss": 0.0003,
      "step": 5330
    },
    {
      "epoch": 9.85239852398524,
      "grad_norm": 0.004880590829998255,
      "learning_rate": 3.455284552845529e-07,
      "loss": 0.0002,
      "step": 5340
    },
    {
      "epoch": 9.870848708487085,
      "grad_norm": 0.005912656895816326,
      "learning_rate": 3.0487804878048784e-07,
      "loss": 0.0003,
      "step": 5350
    },
    {
      "epoch": 9.88929889298893,
      "grad_norm": 0.025090618059039116,
      "learning_rate": 2.642276422764228e-07,
      "loss": 0.0003,
      "step": 5360
    },
    {
      "epoch": 9.907749077490775,
      "grad_norm": 0.017452297732234,
      "learning_rate": 2.2357723577235775e-07,
      "loss": 0.0003,
      "step": 5370
    },
    {
      "epoch": 9.92619926199262,
      "grad_norm": 0.007017238065600395,
      "learning_rate": 1.8292682926829268e-07,
      "loss": 0.0002,
      "step": 5380
    },
    {
      "epoch": 9.944649446494465,
      "grad_norm": 0.005214144475758076,
      "learning_rate": 1.4227642276422766e-07,
      "loss": 0.0002,
      "step": 5390
    },
    {
      "epoch": 9.96309963099631,
      "grad_norm": 0.006517384201288223,
      "learning_rate": 1.0162601626016261e-07,
      "loss": 0.0002,
      "step": 5400
    },
    {
      "epoch": 9.981549815498155,
      "grad_norm": 0.004971297457814217,
      "learning_rate": 6.097560975609757e-08,
      "loss": 0.0002,
      "step": 5410
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.011374194175004959,
      "learning_rate": 2.0325203252032523e-08,
      "loss": 0.0003,
      "step": 5420
    },
    {
      "epoch": 10.0,
      "eval_accuracy": 0.9723247232472325,
      "eval_f1": 0.9717514124293786,
      "eval_loss": 0.21420730650424957,
      "eval_precision": 0.9772727272727273,
      "eval_recall": 0.9662921348314607,
      "eval_runtime": 51.6414,
      "eval_samples_per_second": 20.991,
      "eval_steps_per_second": 1.317,
      "step": 5420
    }
  ],
  "logging_steps": 10,
  "max_steps": 5420,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.14084953604096e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
